{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<module 'torch.backends.mps' from 'C:\\\\Users\\\\joeya\\\\anaconda3\\\\envs\\\\py38-ITS520-Project\\\\lib\\\\site-packages\\\\torch\\\\backends\\\\mps\\\\__init__.py'>\n",
      "devices: 1\n",
      "device:  NVIDIA GeForce RTX 3080\n",
      "device0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080', major=8, minor=6, total_memory=10239MB, multi_processor_count=68)\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    # get number of cuda devices\n",
    "    print(f\"devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"device:  {torch.cuda.get_device_name()}\")\n",
    "    print(f\"device0: {torch.cuda.get_device_properties(0)}\")\n",
    "    print(f\"{torch.cuda.memory_summary()}\")\n",
    "elif torch.backends.mps is not None:\n",
    "    device = torch.device('mps')\n",
    "    print(f\"{torch.mps.current_allocated_memory()}\")\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    # print a warning that cpu is being used\n",
    "    print(\"Warning: Running on CPU. This will be slow.\")\n",
    "print(f\"{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, block_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "\n",
    "        tril_def = torch.tril( torch.ones(block_size, block_size) )  ## [40, 40]\n",
    "        \n",
    "        self.register_buffer(\n",
    "                  'tril', \n",
    "                  tril_def\n",
    "               )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, E = x.shape   ## [batch_size, 40, 512]\n",
    "        \n",
    "        k = self.key(   x )            ## k = (B, T, 64)\n",
    "        q = self.query( x )            ## q = (B, T, 64)\n",
    "\n",
    "        E2 = 64     ## I think this is 64 and not 512\n",
    "        ## (B, T, E) @ (B, E, T)  -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5        \n",
    "        \n",
    "        wei = wei.masked_fill(\n",
    "                      self.tril[:T, :T] == 0, \n",
    "                      float('-inf')\n",
    "        )   \n",
    "        \n",
    "        ## (B, T, T)\n",
    "        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)\n",
    "        wei = self.dropout(   wei   )\n",
    "        \n",
    "        ## perform weighted aggregation of values\n",
    "        \n",
    "        v   = self.value(  x  )   ## x = (B, 40, E)\n",
    "        out = wei @ v             ## (B, T, T) @ (B, T, 64) -> (B, T, 64)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, dropout):         ## 512\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),      ## [512, 4*512]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),      ## [4*512, 512]\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, head_size, block_size, n_embd, dropout):    ## (8, 64)\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(  [ Head(head_size, block_size, n_embd, dropout) for _ in range(n_head) ] )\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)   ## 512, 512\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat(   [ h(x) for h in self.heads ], dim = -1   )\n",
    "        out = self.proj(  out   )\n",
    "        out = self.dropout(   out   )\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_head, block_size, n_embd, dropout):     ## (512, 8)\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head        ## 64\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size, block_size, n_embd, dropout)\n",
    "        self.ffwd = FeedForward( n_embd, dropout)    ## 512\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(     self.ln1(x)      )\n",
    "        x = x + self.ffwd(   self.ln2(x)      )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,vocab_size, n_embd, block_size, n_layer, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   ## [65, 512]\n",
    "        self.pos_emb_table = nn.Embedding(block_size, n_embd)     ## [block, 512]\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[ Block(n_head, block_size, n_embd, dropout) for _ in range(n_layer) ]\n",
    "        )\n",
    "        \n",
    "        self.ln_f    = nn.LayerNorm(  n_embd    )        \n",
    "        self.lm_ffw_head = nn.Linear(n_embd, vocab_size)  ## [512, 65] # FFW Layer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape     ## (Batch, 40)\n",
    "        ## ids and targets are both (B, T) tensors of integers\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))  \n",
    "        \n",
    "        x = tok_emb + pos_emb    ## [B, T, E] or [64, 40, 512]\n",
    "\n",
    "        ## This is the architecture\n",
    "        x = self.blocks(  x  )   ## (B, T, E)        \n",
    "        x = self.ln_f(    x  )   ## (B, T, E)   ## norm\n",
    "        logits = self.lm_ffw_head(x)         ## [B, 40, 65] \n",
    "        \n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, E  = logits.shape\n",
    "        #     logits  = logits.view( B*T, E)\n",
    "        #     targets = targets.view(B*T)\n",
    "        #     loss    = F.cross_entropy(logits, targets)\n",
    "        return logits#, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            ## crop idx to the last block_size tokens\n",
    "            # idx_cond, _loss\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)    ## ## get preds\n",
    "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
    "            probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
    "            idx = torch.cat(  (idx, idx_next), dim=1  )   ## (B, T+1) append sample to running sequence\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    # could do gen_crop\n",
    "    def __init__(self, stride=128, window=None, files=None, samples_per_file=512, normalize=True): # |None\n",
    "        self.files = files\n",
    "\n",
    "        max_length = 1024\n",
    "        if window is not None:\n",
    "            max_length = window\n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        for file in files:\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = df.dropna() # i think axis=1 drops columns\n",
    "            # drop any columns with str\n",
    "            df = df.drop(df.select_dtypes(['object']), axis=1)\n",
    "\n",
    "            # # only get first row\n",
    "            # df = df.iloc[:, 0]\n",
    "\n",
    "            if len(files) == 1:\n",
    "                self.min_val = df.min()\n",
    "                self.max_val = df.max()\n",
    "\n",
    "            if normalize:\n",
    "                norm_df = (df - df.min()) * (50_257-2) / ( df.max() - df.min() )\n",
    "            else:\n",
    "                norm_df = df\n",
    "            n_cols = 1#norm_df.shape[1]\n",
    "\n",
    "            tokens = norm_df.values.flatten().astype(int)\n",
    "            \n",
    "            # Create sequences with sliding window\n",
    "            samples = 0\n",
    "            for i in range(0, len(tokens) - max_length, stride):\n",
    "                sequence = tokens[i:i + max_length]\n",
    "                if len(sequence) == max_length:\n",
    "                    input_sequence = np.array(sequence[:-n_cols])#, dtype=np.int64) # dont include the last token\n",
    "                    target_sequence = np.array(sequence[n_cols:])#, dtype=np.int64) # dont include the first token\n",
    "\n",
    "                    dataset.append({\n",
    "                        'input_ids': input_sequence,\n",
    "                        'labels': target_sequence\n",
    "                    })\n",
    "\n",
    "                samples += 1\n",
    "                if samples_per_file is not None and samples >= samples_per_file: # 1024\n",
    "                    print('max samples from file')\n",
    "                    break\n",
    "            print('samples:', samples)\n",
    "\n",
    "        self.data = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # print(item['input_ids'].shape, item['labels'].shape, torch.ones_like(torch.tensor(item['input_ids'])).shape)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']).to(device),\n",
    "            'labels': torch.tensor(item['labels']).to(device),\n",
    "            'attention_mask': torch.ones_like(torch.tensor(item['input_ids'])).to(device)\n",
    "        }\n",
    "    \n",
    "    def min(self):\n",
    "        if len(self.files) == 1:\n",
    "            return self.min_val\n",
    "        else:\n",
    "            raise Exception(\"Multiple files, use min_val from each file\")\n",
    "    \n",
    "    def max(self):\n",
    "        if len(self.files) == 1:\n",
    "            return self.max_val\n",
    "        else:\n",
    "            raise Exception(\"Multiple files, use max_val from each file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## every id for a given token is embedded to vector of this size\n",
    "n_embd            = 36   \n",
    "n_head            = 12     \n",
    "n_layer           = 6     \n",
    "dropout           = 0.3\n",
    "\n",
    "learning_rate     = 0.0003# 2.5e-4 \n",
    "vocab_size        = 50_257\n",
    "block_size        = 72    # (context) ## N tokens in sequence\n",
    "\n",
    "batch_size        = 8\n",
    "# max_iters         = 512\n",
    "eval_interval     = 256 # 512\n",
    "# eval_iters        = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "\n",
    "class GPTConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size=1024,\n",
    "        vocab_size=50_257,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "class GPTModelForTrainer(PreTrainedModel):\n",
    "    config_class = GPTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # keep model inside because we cant pass it in\n",
    "        self.model = GPTModel(\n",
    "                    vocab_size=self.vocab_size,\n",
    "                    n_embd=self.n_embd,\n",
    "                    block_size=self.block_size,\n",
    "                    n_layer=self.n_layer,\n",
    "                    n_head=self.n_head,\n",
    "                    dropout=self.dropout\n",
    "                )\n",
    "        \n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        logits = self.model(input_ids)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :, :].contiguous()\n",
    "            shift_labels = labels[..., :].contiguous().long()\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                          shift_labels.view(-1))\n",
    "            \n",
    "        return CausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) array of indices in the current context\n",
    "        max_new_tokens: number of tokens to generate\n",
    "        temperature: control randomness (1.0 = neutral, < 1.0 = more deterministic, > 1.0 = more random)\n",
    "        top_k: limit sampling to top k most likely tokens (None = no limit)\n",
    "        \"\"\"\n",
    "        # Make sure model is in eval mode\n",
    "        self.eval()\n",
    "        \n",
    "        # Loop until we generate all requested tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If context length exceeds block_size, crop it\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = self(idx_cond)['logits']\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Optionally crop logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 566\n",
      "samples: 73\n",
      "train size: 566\n",
      "eval size: 73\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# input_files = [str(file) for file in Path('.').glob('00*.csv')]\n",
    "input_files = ['rcalix-clean-train.csv']\n",
    "\n",
    "# TODO handle better maybe\n",
    "# TODO change n-embed\n",
    "train_dataset = GPTDataset(files=input_files, window=block_size, samples_per_file=None, stride=71, normalize=False)\n",
    "eval_dataset = GPTDataset(files=['rcalix-clean-eval.csv'], window=block_size, samples_per_file=None, stride=65, normalize=False)\n",
    "\n",
    "# split the dataset in train and validation set\n",
    "# torch.manual_seed(42)\n",
    "# train_size = int(0.9 * len(dataset))\n",
    "# eval_size = len(dataset) - train_size\n",
    "# train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "print('train size:', len(train_dataset))\n",
    "print('eval size:', len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20214' max='213000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20214/213000 31:19 < 4:58:43, 10.76 it/s, Epoch 284.69/3000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>10.031300</td>\n",
       "      <td>9.747408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>8.694000</td>\n",
       "      <td>9.715718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>8.447200</td>\n",
       "      <td>9.857044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>8.325700</td>\n",
       "      <td>9.915151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>8.146600</td>\n",
       "      <td>9.956540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>7.904400</td>\n",
       "      <td>9.956027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>7.617900</td>\n",
       "      <td>10.018411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>7.303000</td>\n",
       "      <td>10.139639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>6.963900</td>\n",
       "      <td>10.196813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>6.629400</td>\n",
       "      <td>10.303720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2816</td>\n",
       "      <td>6.269900</td>\n",
       "      <td>10.380441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3072</td>\n",
       "      <td>5.920100</td>\n",
       "      <td>10.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3328</td>\n",
       "      <td>5.564800</td>\n",
       "      <td>10.718596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3584</td>\n",
       "      <td>5.212500</td>\n",
       "      <td>10.878384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>4.878500</td>\n",
       "      <td>11.035460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4096</td>\n",
       "      <td>4.540200</td>\n",
       "      <td>11.227013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4352</td>\n",
       "      <td>4.240500</td>\n",
       "      <td>11.389972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4608</td>\n",
       "      <td>3.949100</td>\n",
       "      <td>11.587328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4864</td>\n",
       "      <td>3.672800</td>\n",
       "      <td>11.804092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>3.421700</td>\n",
       "      <td>11.997104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5376</td>\n",
       "      <td>3.177100</td>\n",
       "      <td>12.198840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5632</td>\n",
       "      <td>2.965300</td>\n",
       "      <td>12.522217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5888</td>\n",
       "      <td>2.763100</td>\n",
       "      <td>12.764109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6144</td>\n",
       "      <td>2.575900</td>\n",
       "      <td>12.972634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.416000</td>\n",
       "      <td>13.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6656</td>\n",
       "      <td>2.264800</td>\n",
       "      <td>13.333281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6912</td>\n",
       "      <td>2.135600</td>\n",
       "      <td>13.535703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7168</td>\n",
       "      <td>2.021000</td>\n",
       "      <td>13.851358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7424</td>\n",
       "      <td>1.906300</td>\n",
       "      <td>14.059484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>1.811500</td>\n",
       "      <td>14.282772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7936</td>\n",
       "      <td>1.720400</td>\n",
       "      <td>14.491515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8192</td>\n",
       "      <td>1.638100</td>\n",
       "      <td>14.721644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8448</td>\n",
       "      <td>1.570200</td>\n",
       "      <td>14.946692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8704</td>\n",
       "      <td>1.495000</td>\n",
       "      <td>15.136690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8960</td>\n",
       "      <td>1.436400</td>\n",
       "      <td>15.312303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9216</td>\n",
       "      <td>1.376100</td>\n",
       "      <td>15.521005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9472</td>\n",
       "      <td>1.324100</td>\n",
       "      <td>15.697468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9728</td>\n",
       "      <td>1.277600</td>\n",
       "      <td>15.917103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9984</td>\n",
       "      <td>1.228300</td>\n",
       "      <td>16.203226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10240</td>\n",
       "      <td>1.187300</td>\n",
       "      <td>16.346237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10496</td>\n",
       "      <td>1.145500</td>\n",
       "      <td>16.766270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10752</td>\n",
       "      <td>1.107100</td>\n",
       "      <td>16.797020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11008</td>\n",
       "      <td>1.074300</td>\n",
       "      <td>17.081430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11264</td>\n",
       "      <td>1.035000</td>\n",
       "      <td>17.117376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11520</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>17.535528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11776</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>17.353899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12032</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>17.826073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12288</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>17.891567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12544</td>\n",
       "      <td>0.890100</td>\n",
       "      <td>18.205612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.864100</td>\n",
       "      <td>18.303026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13056</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>18.449686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13312</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>18.757303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13568</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>18.836124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13824</td>\n",
       "      <td>0.773700</td>\n",
       "      <td>19.237566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14080</td>\n",
       "      <td>0.751900</td>\n",
       "      <td>19.173281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14336</td>\n",
       "      <td>0.734700</td>\n",
       "      <td>19.537319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14592</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>19.673279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14848</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>19.841349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15104</td>\n",
       "      <td>0.679300</td>\n",
       "      <td>19.936045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15360</td>\n",
       "      <td>0.661600</td>\n",
       "      <td>20.220915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15616</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>20.367853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15872</td>\n",
       "      <td>0.627200</td>\n",
       "      <td>20.483103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16128</td>\n",
       "      <td>0.613800</td>\n",
       "      <td>20.623840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16384</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>20.807598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16640</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>20.905542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16896</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>21.095463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17152</td>\n",
       "      <td>0.553400</td>\n",
       "      <td>21.096781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17408</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>21.425802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17664</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>21.661564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17920</td>\n",
       "      <td>0.515900</td>\n",
       "      <td>21.626089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18176</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>21.766590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18432</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>22.087589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18688</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>22.189095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18944</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>22.313982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>22.479809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19456</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>22.616514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19712</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>22.823069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19968</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>22.860310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "output_dir = Path('models') / time.strftime(\"%y-%m-%d_%H\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3000,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_steps=eval_interval,\n",
    "    save_steps=eval_interval,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=learning_rate,\n",
    "    # fp16=True,  # if you want to use mixed precision training\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=eval_interval,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# Wrap the model\n",
    "config = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model_for_trainer = GPTModelForTrainer(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_trainer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights\n",
    "# Replace 'checkpoint-XXX' with the specific checkpoint you want to load\n",
    "checkpoint_path = \"./gpt_model/checkpoint-512\"  # or whatever your output_dir/checkpoint-XXX is\n",
    "\n",
    "import os\n",
    "latest_model = f\"models/{sorted(os.listdir('models'))[-1]}\"\n",
    "last_checkpoint = max([f\"{latest_model}/{f}\" for f in os.listdir(latest_model) if os.path.isdir(f\"{latest_model}/{f}\")], key=os.path.getmtime)\n",
    "\n",
    "print(last_checkpoint)\n",
    "model_for_trainer = GPTModelForTrainer.from_pretrained(last_checkpoint)\n",
    "\n",
    "# Put model in evaluation mode if you're going to use it for inference\n",
    "model_for_trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO good enough for now\n",
    "device = 'cpu'\n",
    "\n",
    "# Generate text\n",
    "output = model_for_trainer.generate(\n",
    "    eval_dataset[0]['input_ids'].unsqueeze(0),\n",
    "    # max_length=512,\n",
    "    max_new_tokens=1,\n",
    "    # temperature=0.7,\n",
    "    # num_return_sequences=1,\n",
    "    # pad_token_id=vocab_size - 1,\n",
    "    # do_sample=True\n",
    ")\n",
    "\n",
    "torch.set_printoptions(profile='full')\n",
    "print('start')\n",
    "print('real',eval_dataset[0]['labels'][:32])\n",
    "print('pred',output[0][1:33])\n",
    "print('middle (1020:1050)')\n",
    "print('real',eval_dataset[0]['labels'][1000:1050])\n",
    "print('pred',output[0][1001:1051])\n",
    "print('end')\n",
    "print('real',eval_dataset[0]['labels'][-32:])\n",
    "print('pred',output[0][-32:])\n",
    "torch.set_printoptions(profile='default')\n",
    "\n",
    "# create scatter plot\n",
    "n = 100\n",
    "m = 1\n",
    "\n",
    "# not sure how to line up the real and pred when using different size. so i will just change dataset size\n",
    "real = torch.full_like(torch.zeros(n), eval_dataset[0]['labels'][-1])\n",
    "pred = torch.zeros(n)\n",
    "\n",
    "pbar = tqdm(range(0, n, m), total=n)\n",
    "for i in pbar:\n",
    "    output = model_for_trainer.generate(\n",
    "        eval_dataset[0]['input_ids'][:block_size].unsqueeze(0),\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "\n",
    "    for j in range(m): # -1\n",
    "        pred[i+j] = output[j][-1].cpu() # have to do this to get the value\n",
    "    pbar.update(m)\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(range(n), real, label='real')\n",
    "plt.scatter(range(n), pred, label='pred')\n",
    "plt.title('Repeated Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame({'Predictions': pred})\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(y='Predictions', data=df)\n",
    "plt.title(f\"Distribution of  Predictions\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"real: shape: {real.shape}, goal val: {real[0]}\")\n",
    "print(f\"min: {min(pred)}, max: {max(pred)}, mean: {torch.mean(pred)}, std: {torch.std(pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scatter plot\n",
    "n = 100\n",
    "m = 1\n",
    "\n",
    "if n >= len(eval_dataset)-1:\n",
    "    n = len(eval_dataset)-1\n",
    "\n",
    "# not sure how to line up the real and pred when using different size. so i will just change dataset size\n",
    "real = torch.zeros(n)\n",
    "pred = torch.zeros(n)\n",
    "\n",
    "pbar = tqdm(range(0, n, m), total=n)\n",
    "for i in pbar:\n",
    "    output = model_for_trainer.generate(\n",
    "        eval_dataset[i]['input_ids'][:block_size].unsqueeze(0),\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "\n",
    "    for j in range(m): # -1\n",
    "        real[i+j] = eval_dataset[i]['labels'][-1]\n",
    "        pred[i+j] = output[j][-1].cpu() # have to do this to get the value\n",
    "    pbar.update(m)\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(range(n), real, label='real')\n",
    "plt.scatter(range(n), pred, label='pred')\n",
    "plt.title('Single Step Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.mean((real - pred) ** 2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim percent\n",
    "max_diff = torch.max(torch.abs(real))  # Maximum possible difference\n",
    "mae = torch.mean(torch.abs(real - pred))\n",
    "\n",
    "print(100 * (1 - mae/max_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO cant do this, because they have different norms\n",
    "test_dataset = GPTDataset(files=['rcalix-clean-test.csv'], window=block_size+64, samples_per_file=1, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_for_trainer.generate(\n",
    "    test_dataset[0]['input_ids'][:block_size].unsqueeze(0),\n",
    "    max_new_tokens=64,\n",
    ")\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(range(64), test_dataset[0]['labels'][-64:], label='real')\n",
    "plt.scatter(range(64), output[0][block_size:], label='pred')\n",
    "plt.plot(range(64), test_dataset[0]['labels'][-64:], label=\"real\", color='#8884d8')\n",
    "plt.plot(range(64), output[0][block_size:], label=\"pred\", color='#ff7300')\n",
    "plt.title('Sequence Extrapolation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # go as far as we can\n",
    "\n",
    "real = torch.zeros(block_size+64-1)\n",
    "pred = torch.zeros(block_size+64-1)\n",
    "input_ids = test_dataset[0]['input_ids'][:block_size]\n",
    "\n",
    "# could do it this way, or keep calling it with the last block i:i+block_size\n",
    "i = 0\n",
    "err = 0\n",
    "overs = []\n",
    "df_over = pd.DataFrame(columns=['i', 'real', 'pred-0', 'loss-0', 'pred-1', 'loss-1', 'pred-2', 'loss-2'])\n",
    "display_handle = display(df_over, display_id=True)\n",
    "while err < 0.2 and i < 64-1:\n",
    "    # print(i, end=': ')\n",
    "    real[i] = test_dataset[0]['labels'][block_size+i-1]\n",
    "    over = {'i': i, 'real': real[i].item() }\n",
    "    for j in range(3):\n",
    "        output = model_for_trainer.generate(\n",
    "            input_ids.unsqueeze(0),\n",
    "            max_new_tokens=1,\n",
    "        )\n",
    "\n",
    "        pred[i] = output[0].cpu()[-1]\n",
    "        # print(label, out)\n",
    "        # Calculate relative error as |real - pred| / max(|real|, |pred|)\n",
    "        # This handles cases where real or pred could be zero or very different magnitudes\n",
    "        numerator = torch.abs(real[i] - pred[i])\n",
    "        denominator = torch.maximum(torch.abs(real[i]), torch.abs(pred[i]))\n",
    "        # Add small epsilon to prevent division by zero if both values are 0\n",
    "        epsilon = 1e-10\n",
    "        relative_error = numerator / (denominator + epsilon)\n",
    "        # print(real[i], pred[i], end=' ')\n",
    "        # print(relative_error, end='\\t')\n",
    "        over[f\"pred-{j}\"] = pred[i].item()\n",
    "        over[f\"loss-{j}\"] = relative_error.item()\n",
    "\n",
    "    input_ids = torch.cat((input_ids[1:], pred[i].long().unsqueeze(0)))\n",
    "    # print()\n",
    "    overs.append(over)\n",
    "    df_over = pd.DataFrame(overs)\n",
    "\n",
    "    # bad to keep it, but its the last one\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display_handle.update(df_over)\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(range(i), real[:i], label='real')\n",
    "plt.scatter(range(i), pred[:i], label='pred')\n",
    "plt.plot(range(i), real[:i], label=\"real\", color='#8884d8')\n",
    "plt.plot(range(i), pred[:i], label=\"pred\", color='#ff7300')\n",
    "plt.title('Sequence Extrapolation Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(df_over['i'], df_over['loss-0'], label=f\"Loss 0\", color='#8884d8')\n",
    "plt.plot(df_over['i'], df_over['loss-1'], label=f\"Loss 1\", color='#82ca9d')\n",
    "plt.plot(df_over['i'], df_over['loss-1'], label=f\"Loss 2\", color='#ff7300')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
