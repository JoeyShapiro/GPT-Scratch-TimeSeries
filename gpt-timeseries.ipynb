{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<module 'torch.backends.mps' from '/Users/oniichan/anaconda3/envs/its530_py38/lib/python3.8/site-packages/torch/backends/mps/__init__.py'>\n",
      "1558722304\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    # get number of cuda devices\n",
    "    print(f\"devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"device:  {torch.cuda.get_device_name()}\")\n",
    "    print(f\"device0: {torch.cuda.get_device_properties(0)}\")\n",
    "    print(f\"{torch.cuda.memory_summary()}\")\n",
    "elif torch.backends.mps is not None:\n",
    "    device = torch.device('mps')\n",
    "    print(f\"{torch.mps.current_allocated_memory()}\")\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    # print a warning that cpu is being used\n",
    "    print(\"Warning: Running on CPU. This will be slow.\")\n",
    "print(f\"{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, block_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "\n",
    "        tril_def = torch.tril( torch.ones(block_size, block_size) )  ## [40, 40]\n",
    "        \n",
    "        self.register_buffer(\n",
    "                  'tril', \n",
    "                  tril_def\n",
    "               )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, E = x.shape   ## [batch_size, 40, 512]\n",
    "        \n",
    "        k = self.key(   x )            ## k = (B, T, 64)\n",
    "        q = self.query( x )            ## q = (B, T, 64)\n",
    "\n",
    "        E2 = 64     ## I think this is 64 and not 512\n",
    "        ## (B, T, E) @ (B, E, T)  -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5        \n",
    "        \n",
    "        wei = wei.masked_fill(\n",
    "                      self.tril[:T, :T] == 0, \n",
    "                      float('-inf')\n",
    "        )   \n",
    "        \n",
    "        ## (B, T, T)\n",
    "        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)\n",
    "        wei = self.dropout(   wei   )\n",
    "        \n",
    "        ## perform weighted aggregation of values\n",
    "        \n",
    "        v   = self.value(  x  )   ## x = (B, 40, E)\n",
    "        out = wei @ v             ## (B, T, T) @ (B, T, 64) -> (B, T, 64)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, dropout):         ## 512\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),      ## [512, 4*512]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),      ## [4*512, 512]\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, head_size, block_size, n_embd, dropout):    ## (8, 64)\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(  [ Head(head_size, block_size, n_embd, dropout) for _ in range(n_head) ] )\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)   ## 512, 512\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat(   [ h(x) for h in self.heads ], dim = -1   )\n",
    "        out = self.proj(  out   )\n",
    "        out = self.dropout(   out   )\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_head, block_size, n_embd, dropout):     ## (512, 8)\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head        ## 64\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size, block_size, n_embd, dropout)\n",
    "        self.ffwd = FeedForward( n_embd, dropout)    ## 512\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(     self.ln1(x)      )\n",
    "        x = x + self.ffwd(   self.ln2(x)      )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, n_embd, block_size, n_layer, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(112, n_embd)   ## [65, 512]\n",
    "        self.pos_emb_table = nn.Embedding(block_size, n_embd)     ## [block, 512]\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[ Block(n_head, block_size, n_embd, dropout) for _ in range(n_layer) ]\n",
    "        )\n",
    "        \n",
    "        self.ln_f    = nn.LayerNorm(  n_embd    )        \n",
    "        self.lm_ffw_head = nn.Linear(n_embd, 112)  ## [512, 65] # FFW Layer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape     ## (Batch, 40)\n",
    "        ## ids and targets are both (B, T) tensors of integers\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)      \n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))  \n",
    "        \n",
    "        x = tok_emb + pos_emb    ## [B, T, E] or [64, 40, 512]\n",
    "\n",
    "        ## This is the architecture\n",
    "        x = self.blocks(  x  )   ## (B, T, E)        \n",
    "        x = self.ln_f(    x  )   ## (B, T, E)   ## norm\n",
    "        logits = self.lm_ffw_head(x)         ## [B, 40, 65] \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, E  = logits.shape\n",
    "            logits  = logits.view( B*T, E)\n",
    "            targets = targets.view(B*T)\n",
    "            loss    = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            ## crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _loss = self(idx_cond)    ## ## get preds\n",
    "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
    "            probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
    "            idx = torch.cat(  (idx, idx_next), dim=1  )   ## (B, T+1) append sample to running sequence\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7855</td>\n",
       "      <td>1.6110</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>0.634196</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.525486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7818</td>\n",
       "      <td>1.6100</td>\n",
       "      <td>0.861104</td>\n",
       "      <td>0.633513</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.523972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7867</td>\n",
       "      <td>1.6293</td>\n",
       "      <td>0.861030</td>\n",
       "      <td>0.648508</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.5973</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7860</td>\n",
       "      <td>1.6370</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.650618</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>0.523834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7849</td>\n",
       "      <td>1.6530</td>\n",
       "      <td>0.861995</td>\n",
       "      <td>0.656254</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>0.527426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1         2         3         4         5       6         7\n",
       "0  0.7855  1.6110  0.861698  0.634196  0.211242  0.006838  0.5930  0.525486\n",
       "1  0.7818  1.6100  0.861104  0.633513  0.211242  0.006863  0.5940  0.523972\n",
       "2  0.7867  1.6293  0.861030  0.648508  0.211242  0.006975  0.5973  0.526316\n",
       "3  0.7860  1.6370  0.862069  0.650618  0.211242  0.006953  0.5970  0.523834\n",
       "4  0.7849  1.6530  0.861995  0.656254  0.211242  0.006940  0.5985  0.527426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24525 22368 25833 ... 16643 30769 27202] (60704,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('exchange_rate.txt', header=None)\n",
    "display(df.head())\n",
    "\n",
    "norm_df = (df - df.min()) * (50_257-2) / ( df.max() - df.min() )\n",
    "tokens = norm_df.values.flatten().astype(int)\n",
    "print(tokens, tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokens[:2056], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## every id for a given token is embedded to vector of this size\n",
    "n_embd            = 768        # GPT-2\n",
    "n_head            = 12         # GPT-2\n",
    "n_layer           = 12         # GPT-2\n",
    "dropout           = 0.1        # GPT-2\n",
    "\n",
    "learning_rate     = 2.5e-4     # GPT-2\n",
    "vocab_size        = 50_257     # GPT-2 50_257\n",
    "block_size        = 1024       # GPT-2 (context) ## N tokens in sequence\n",
    "\n",
    "batch_size        = 64\n",
    "# max_iters         = 512\n",
    "eval_interval     = 512\n",
    "# eval_iters        = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(\n",
    "                    n_embd=n_embd,\n",
    "                    block_size=block_size,\n",
    "                    n_layer=n_layer,\n",
    "                    n_head=n_head,\n",
    "                    dropout=dropout\n",
    "                ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "\n",
    "class GPTConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size=40,\n",
    "        vocab_size=98,\n",
    "        n_embd=512,\n",
    "        n_head=8,\n",
    "        n_layer=6,\n",
    "        dropout=0.2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "class GPTModelForTrainer(PreTrainedModel):\n",
    "    def __init__(self, config, gpt_model):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = gpt_model\n",
    "        \n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        logits = self.model(input_ids.unsqueeze(0))\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                          shift_labels.view(-1))\n",
    "            \n",
    "        print(labels)\n",
    "        return CausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oniichan/anaconda3/envs/its530_py38/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[-32.9011, -31.2024, -34.6622,  ..., -39.4867, -39.8731, -32.2387],\n",
      "        [-55.5207, -53.4285, -56.4767,  ..., -68.1539, -66.7708, -58.6006],\n",
      "        [-61.7969, -60.5386, -59.5503,  ..., -75.3206, -72.7731, -65.5706]],\n",
      "       grad_fn=<MmBackward0>), past_key_values=((tensor([[[[-1.1621,  2.1424,  0.9899,  ..., -1.2493, -0.6088,  1.6558],\n",
      "          [-1.7831,  2.2802,  2.5158,  ..., -0.2474, -1.4704,  1.5416],\n",
      "          [-0.8020,  2.3662,  3.0656,  ..., -0.8665, -1.0335,  1.7610]],\n",
      "\n",
      "         [[-0.3037,  0.1982, -0.4324,  ..., -0.1011,  2.3475,  0.6145],\n",
      "          [ 0.1066, -0.6560, -0.1682,  ...,  1.6234,  4.6732,  1.6981],\n",
      "          [-0.1469, -2.1942, -1.2178,  ..., -1.1906,  4.2506,  0.3419]],\n",
      "\n",
      "         [[ 0.0409, -0.3168,  0.8519,  ..., -1.6563, -1.4363,  0.7900],\n",
      "          [ 1.2763,  0.1916,  0.7094,  ..., -1.8491,  1.0007,  1.8073],\n",
      "          [ 3.3321,  0.5271,  1.5881,  ..., -2.8760,  0.2359,  1.7286]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4781, -0.1036, -0.2554,  ...,  0.1891,  0.7368,  0.5510],\n",
      "          [-0.5316, -0.0316, -0.4302,  ...,  0.8687,  0.3834,  0.5224],\n",
      "          [-0.2135, -0.1710,  0.0179,  ...,  1.1507, -0.0194,  1.2237]],\n",
      "\n",
      "         [[ 1.1598,  1.2693, -0.4913,  ..., -0.2153,  1.0413, -1.3457],\n",
      "          [ 1.1488,  0.9189, -0.3928,  ..., -0.7797,  1.4032, -0.2486],\n",
      "          [ 1.2714,  0.8630,  0.0092,  ..., -0.2769,  0.7797, -0.9545]],\n",
      "\n",
      "         [[ 0.9168,  0.0855,  0.5821,  ..., -0.4090,  0.1881,  1.9430],\n",
      "          [ 0.2358,  0.1036,  0.9853,  ..., -0.2530,  0.1831,  0.9657],\n",
      "          [-0.4602, -0.1932,  0.7204,  ..., -0.3215,  0.4173,  0.9872]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.4607e-01,  5.0661e-02,  2.1019e-02,  ..., -5.7896e-02,\n",
      "            1.8751e-01,  1.0744e-01],\n",
      "          [-1.3257e-01,  2.2813e-01, -9.6158e-02,  ...,  4.7321e-02,\n",
      "           -3.0846e-01, -3.8431e-02],\n",
      "          [ 4.1702e-02, -4.1829e-02,  2.2552e-01,  ...,  6.3606e-01,\n",
      "            1.6073e-01,  2.3105e-01]],\n",
      "\n",
      "         [[ 5.3319e-01,  1.6739e-01, -1.8503e-01,  ..., -6.1610e-01,\n",
      "           -2.0244e-01,  1.7564e-01],\n",
      "          [ 3.1870e-01, -1.1490e-01, -6.8814e-03,  ...,  1.0859e-02,\n",
      "            5.5278e-01, -1.7371e-01],\n",
      "          [ 2.3433e-01, -4.7236e-02,  1.0021e-01,  ..., -3.2008e-01,\n",
      "            4.2239e-01, -1.2330e-01]],\n",
      "\n",
      "         [[-1.4561e-02,  1.2557e-01, -5.5501e-02,  ..., -1.5678e-01,\n",
      "            2.0515e-01, -4.1502e-02],\n",
      "          [-1.3487e-01,  1.5685e-01, -2.1005e-02,  ...,  6.6801e-02,\n",
      "            1.3690e-01,  1.1897e-02],\n",
      "          [ 2.3807e-01, -1.5784e-01, -1.7674e-01,  ...,  3.5388e-01,\n",
      "           -2.1289e-02, -1.9908e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9795e-01,  3.1703e-02,  5.8946e-02,  ..., -1.7592e-01,\n",
      "           -1.2736e-01, -2.0584e-02],\n",
      "          [-2.2264e-01,  5.8093e-01, -9.4764e-02,  ..., -2.2327e-01,\n",
      "            2.7295e-01,  7.7845e-02],\n",
      "          [ 6.4711e-02,  9.7038e-01,  3.1855e-02,  ..., -2.9324e-01,\n",
      "            4.1885e-02, -7.7138e-03]],\n",
      "\n",
      "         [[ 8.7687e-02, -6.8696e-02, -4.0954e-03,  ...,  1.7411e-01,\n",
      "            1.3293e-01, -1.2079e-01],\n",
      "          [-2.2905e-01,  3.4694e-01,  1.6894e-01,  ..., -3.2815e-01,\n",
      "            5.1061e-02,  8.4314e-02],\n",
      "          [-1.0721e-01,  2.3567e-01, -3.5386e-02,  ..., -1.5127e-01,\n",
      "           -2.2830e-01, -1.4558e-01]],\n",
      "\n",
      "         [[ 3.6397e-02, -3.4056e-01,  1.5237e-01,  ...,  4.8770e-02,\n",
      "           -2.3677e-01, -3.5391e-03],\n",
      "          [ 1.5324e-01,  4.4967e-02,  5.3123e-04,  ...,  1.1628e-01,\n",
      "            3.5223e-01,  1.1112e-01],\n",
      "          [ 6.8347e-02,  5.9131e-02,  8.8776e-03,  ...,  2.8421e-01,\n",
      "            2.4362e-04,  2.6779e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 6.7759e-02,  1.9347e+00, -1.7453e+00,  ...,  1.3850e+00,\n",
      "           -1.4229e+00,  1.4344e+00],\n",
      "          [ 1.3721e+00, -2.2118e-01, -9.0592e-01,  ..., -1.8923e-01,\n",
      "           -1.1695e+00, -1.8068e-02],\n",
      "          [ 1.1361e+00,  3.0549e-01, -1.2663e+00,  ..., -3.6413e-01,\n",
      "           -1.9926e+00,  1.8554e-02]],\n",
      "\n",
      "         [[-1.0714e+00, -5.5637e-01, -6.6282e-01,  ..., -4.2968e-01,\n",
      "            9.1451e-01, -6.2736e-01],\n",
      "          [-9.1589e-01, -1.2643e+00, -1.2414e+00,  ..., -7.1077e-01,\n",
      "           -4.7153e-01,  1.6048e-01],\n",
      "          [-5.3117e-01, -1.1833e+00, -1.3115e+00,  ..., -1.2098e+00,\n",
      "            6.6100e-01,  9.9059e-02]],\n",
      "\n",
      "         [[ 2.0875e-01,  4.2339e-03, -6.6426e-02,  ..., -1.3486e+00,\n",
      "            1.7642e-01, -1.7596e-01],\n",
      "          [ 1.8661e-01,  2.1755e-01, -2.6050e-01,  ..., -1.3009e+00,\n",
      "           -1.4346e-01, -1.3324e-01],\n",
      "          [ 8.7040e-02, -3.0240e-01, -2.7108e-01,  ..., -9.3408e-01,\n",
      "           -1.9545e-01, -4.5355e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.9905e-05, -1.4513e-01, -7.5795e-01,  ..., -4.3100e-01,\n",
      "            8.0919e-01, -7.6411e-01],\n",
      "          [-1.4900e+00,  6.8207e-01,  2.7925e+00,  ...,  1.0800e-02,\n",
      "           -1.9276e+00, -2.0115e+00],\n",
      "          [-9.2946e-01,  1.3560e-01,  2.1336e+00,  ..., -2.9529e-01,\n",
      "           -9.2958e-01, -1.2201e+00]],\n",
      "\n",
      "         [[-1.1805e+00, -2.6913e+00,  2.0074e-01,  ...,  1.8159e+00,\n",
      "            1.5804e+00, -1.6540e+00],\n",
      "          [ 5.1918e-02,  9.2872e-01, -4.5710e-01,  ..., -1.2107e+00,\n",
      "            6.4258e-01,  5.1733e-02],\n",
      "          [-2.4929e-01,  3.4646e-01, -5.4813e-01,  ..., -9.8615e-01,\n",
      "            7.3104e-01,  1.5269e-01]],\n",
      "\n",
      "         [[ 6.6905e-01,  1.8148e+00,  7.4696e-01,  ..., -6.8862e-01,\n",
      "           -4.7587e-01,  5.5460e-01],\n",
      "          [ 7.3773e-01,  2.6339e+00,  2.5120e+00,  ...,  6.4067e-01,\n",
      "           -8.4120e-01, -6.9856e-01],\n",
      "          [ 1.3577e-01,  1.3079e-01,  2.0399e+00,  ...,  6.8124e-01,\n",
      "           -2.5726e-01, -1.1065e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 2.3935e-01,  3.5299e-02, -1.2105e-01,  ..., -6.8110e-02,\n",
      "           -1.9120e-01, -3.1445e-01],\n",
      "          [ 6.5970e-01,  8.4422e-02, -7.5095e-01,  ...,  2.0143e-01,\n",
      "           -3.8422e-02, -6.2091e-02],\n",
      "          [-8.2311e-01, -3.2821e-01, -4.0445e-01,  ..., -1.1871e-01,\n",
      "           -9.3435e-02,  8.4113e-01]],\n",
      "\n",
      "         [[ 1.0068e-01, -2.3901e-01, -7.3090e-03,  ...,  1.3319e-01,\n",
      "           -2.7163e-01,  3.6159e-02],\n",
      "          [-5.3203e-01,  1.2334e-01, -1.6062e-01,  ..., -3.5355e-01,\n",
      "            6.1163e-02, -1.2042e+00],\n",
      "          [-2.6937e-01,  4.0917e-01, -2.3897e-01,  ..., -6.7850e-01,\n",
      "            2.7121e-01, -5.9496e-01]],\n",
      "\n",
      "         [[-1.7923e-02,  1.2062e-01,  2.3242e-02,  ..., -5.9322e-01,\n",
      "            2.0180e-02, -1.3352e-02],\n",
      "          [ 5.3803e-01,  2.5652e-01,  4.9138e-01,  ..., -4.0444e-01,\n",
      "            1.2252e-01,  1.6247e-01],\n",
      "          [ 6.6634e-01,  2.2306e-01,  2.9145e-01,  ..., -4.3416e-01,\n",
      "            1.5592e-01, -2.1784e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9923e-02,  6.8822e-01,  4.2077e-03,  ...,  5.9994e-03,\n",
      "           -9.4965e-01, -2.2555e-01],\n",
      "          [-1.4920e-01, -5.7847e-01, -3.2453e-01,  ..., -1.4988e-02,\n",
      "           -1.0617e+00,  1.3976e-01],\n",
      "          [-1.2029e-01, -7.5246e-01,  3.3103e-01,  ...,  5.3372e-01,\n",
      "           -6.5963e-01,  1.3533e-01]],\n",
      "\n",
      "         [[-8.4611e-02,  2.6249e-02, -1.7769e-02,  ...,  3.4342e-01,\n",
      "           -3.6769e+00, -4.8495e-02],\n",
      "          [ 9.0060e-02, -1.9258e-01,  1.7040e-01,  ...,  5.0047e-02,\n",
      "           -1.4490e-01,  1.8259e-01],\n",
      "          [ 4.9015e-02, -8.6461e-02,  1.9363e-01,  ...,  2.3594e-02,\n",
      "           -8.3169e-02,  1.1877e-01]],\n",
      "\n",
      "         [[ 1.5473e-01, -1.6330e-01,  2.8277e-03,  ..., -1.1219e-01,\n",
      "            1.6896e-01, -2.9684e-02],\n",
      "          [-1.7471e-01, -2.4422e-01,  7.3945e-02,  ..., -1.6995e-01,\n",
      "            2.6279e-01, -2.7746e-01],\n",
      "          [-2.7948e-01, -3.0806e-01, -1.2440e-01,  ..., -2.2651e-01,\n",
      "            1.1005e-01, -3.7842e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.1105, -1.1727,  0.2974,  ..., -0.6675, -0.0711, -0.0579],\n",
      "          [ 0.0783, -2.5804, -0.4330,  ..., -0.1537, -0.3485, -0.8429],\n",
      "          [ 0.4285, -2.0437,  0.4519,  ...,  0.1499, -1.0634, -1.0173]],\n",
      "\n",
      "         [[-0.4495,  0.3910, -0.4514,  ...,  1.2141, -0.4817, -0.5234],\n",
      "          [-1.9687,  0.0871, -2.7329,  ..., -0.4643,  1.0200, -1.2836],\n",
      "          [-1.6320, -1.2102, -3.0084,  ..., -0.1052,  0.6288, -0.7723]],\n",
      "\n",
      "         [[ 1.2703,  3.0332,  3.8094,  ...,  0.6496,  1.7541, -0.7071],\n",
      "          [-2.8646,  1.9892, -1.6209,  ..., -2.5802,  3.6040, -0.6949],\n",
      "          [-4.2759,  0.6153, -2.7296,  ..., -2.2737,  2.3277, -0.0994]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3646, -2.7863, -2.6834,  ...,  0.9005,  0.4739,  2.7269],\n",
      "          [-1.1624,  1.9412,  0.1716,  ...,  0.4359, -2.0577,  0.1594],\n",
      "          [-2.5052,  2.0746,  0.6835,  ..., -0.2181, -1.5314, -0.5938]],\n",
      "\n",
      "         [[ 1.7649,  0.4034,  0.9259,  ...,  0.0268, -1.0186, -0.2717],\n",
      "          [ 2.0572,  0.4211,  1.4808,  ...,  0.1775, -2.8605, -2.1054],\n",
      "          [ 2.0013,  0.3832,  1.3475,  ...,  0.0197, -2.1696, -2.1077]],\n",
      "\n",
      "         [[-0.2503,  0.1818, -0.5761,  ...,  0.3019,  0.2974,  0.2088],\n",
      "          [-0.4786,  0.7546,  0.1196,  ...,  0.1544,  0.9252, -0.0386],\n",
      "          [ 0.0774,  0.3938,  0.2772,  ...,  0.0561,  1.3112, -0.0743]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-2.1638e-02, -1.4774e-02, -1.7797e-01,  ...,  2.2120e-02,\n",
      "            4.8053e-03, -5.5826e-01],\n",
      "          [ 8.7734e-01,  2.7592e-01, -7.1363e-02,  ...,  6.8448e-01,\n",
      "           -2.6987e-01,  6.5361e-01],\n",
      "          [ 3.9125e-01, -2.4134e-02, -3.7177e-01,  ...,  4.5631e-01,\n",
      "            1.8854e-01,  1.0122e+00]],\n",
      "\n",
      "         [[ 5.1857e-02, -3.4768e-02,  3.1720e-02,  ..., -7.0808e-03,\n",
      "           -2.3854e-03,  1.6675e-02],\n",
      "          [ 1.7826e-01, -1.2651e-01, -4.3028e-02,  ..., -5.1550e-01,\n",
      "           -5.8387e-01, -4.1001e-01],\n",
      "          [-3.5189e-01,  1.7464e-01,  4.7355e-02,  ..., -6.9717e-01,\n",
      "           -5.4223e-01, -4.9208e-01]],\n",
      "\n",
      "         [[ 9.3413e-03, -8.1256e-01, -4.2147e-02,  ...,  5.0464e-02,\n",
      "           -5.7203e-04, -3.6057e-02],\n",
      "          [-6.0622e-03, -1.2955e+00,  5.2869e-02,  ..., -1.0847e+00,\n",
      "           -1.7196e-02,  4.3429e-01],\n",
      "          [ 4.6859e-01, -1.7399e+00,  4.9771e-01,  ..., -5.0553e-01,\n",
      "           -3.0577e-01,  4.1933e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7579e-02, -1.1849e-01,  1.3298e+00,  ..., -5.2191e-02,\n",
      "            1.9064e-01,  5.0287e-03],\n",
      "          [ 6.5942e-01,  2.7356e-01,  1.9841e+00,  ..., -1.0354e-01,\n",
      "            3.0025e-01, -7.9377e-01],\n",
      "          [ 4.7340e-01,  2.4737e-01,  1.6839e+00,  ..., -4.8356e-01,\n",
      "            5.5980e-01, -5.4868e-01]],\n",
      "\n",
      "         [[ 8.7096e-03, -4.8807e-02, -1.7105e-01,  ...,  9.3605e-02,\n",
      "            1.2649e-01,  1.5730e-01],\n",
      "          [ 1.1811e-01,  5.3412e-01, -6.0110e-01,  ..., -7.1366e-01,\n",
      "           -1.1086e-01, -6.8169e-01],\n",
      "          [ 1.2337e-01, -1.0480e-01, -9.8357e-01,  ..., -4.0952e-01,\n",
      "           -4.6767e-01, -6.3147e-01]],\n",
      "\n",
      "         [[ 5.0770e-02,  1.7951e-02,  4.5884e-02,  ..., -3.4245e-02,\n",
      "            2.2487e-01,  1.8763e-02],\n",
      "          [-5.7319e-01,  4.0811e-01,  4.0905e-01,  ..., -2.4524e-01,\n",
      "           -2.0615e+00, -2.4872e-01],\n",
      "          [-1.4554e-01, -2.1994e-01, -2.1198e-01,  ..., -1.2995e-01,\n",
      "           -2.1845e+00, -1.2607e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0413, -0.2108,  0.1637,  ..., -0.8878,  0.7458, -1.2035],\n",
      "          [ 1.0883, -2.3308,  1.2379,  ..., -0.2909, -2.0996,  0.5676],\n",
      "          [-1.2900, -1.0124,  1.1002,  ...,  1.6027, -1.7494,  2.6136]],\n",
      "\n",
      "         [[ 0.7871,  0.2186,  0.0103,  ..., -0.1747, -1.1213, -0.1826],\n",
      "          [-0.7560, -1.3352, -0.1232,  ..., -0.6432,  2.9462,  0.8351],\n",
      "          [-0.8024, -1.5435,  0.0105,  ..., -1.7811,  4.3004,  1.1207]],\n",
      "\n",
      "         [[ 0.3423, -0.3517, -0.3218,  ...,  0.3223,  1.4391,  0.2641],\n",
      "          [-0.5241, -5.6296, -1.5983,  ..., -3.2101, -2.1014, -4.4704],\n",
      "          [-0.6954, -5.8310, -1.3567,  ..., -5.8191, -2.4124, -5.0375]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2420,  1.7846,  0.5132,  ...,  0.2597,  0.4391, -1.6788],\n",
      "          [ 0.9945, -5.4542,  0.4416,  ..., -3.7075, -1.0168,  4.7431],\n",
      "          [-0.2776, -6.7612,  0.5788,  ..., -3.0444, -2.0415,  5.0434]],\n",
      "\n",
      "         [[ 0.0597, -0.0386,  0.1327,  ..., -0.1022, -0.0878, -0.1372],\n",
      "          [ 0.8013,  0.2328,  0.2907,  ..., -0.5317,  0.0116, -1.1318],\n",
      "          [ 0.0702, -0.3921, -0.2334,  ...,  0.2982, -0.7779, -0.4170]],\n",
      "\n",
      "         [[ 0.4007, -0.0575,  1.9017,  ..., -0.2471, -0.2060, -0.9828],\n",
      "          [ 2.3999,  0.8252, -2.7764,  ...,  0.9814, -0.9141,  2.8430],\n",
      "          [ 2.9607,  1.2915, -3.6165,  ...,  1.4537,  0.0962,  4.5073]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0463,  0.0691, -0.0033,  ...,  0.0050,  0.1030,  0.0375],\n",
      "          [ 0.6403, -1.6527,  0.1704,  ...,  0.2856,  0.5065, -1.1517],\n",
      "          [ 0.5362, -1.4926,  0.0256,  ...,  0.2520,  0.0296, -0.1493]],\n",
      "\n",
      "         [[-0.0365,  0.0071,  0.0845,  ..., -0.0390, -0.0338, -0.0438],\n",
      "          [ 0.2323, -0.3214, -0.1788,  ..., -0.5112, -0.8578,  0.0274],\n",
      "          [ 0.1264,  0.1784, -0.0523,  ...,  0.5182, -0.0790, -0.4274]],\n",
      "\n",
      "         [[ 0.0378, -0.1103, -0.0616,  ..., -0.0277,  0.0852, -0.1452],\n",
      "          [-0.2074, -0.0690, -0.4649,  ..., -0.0709,  0.2267, -0.1452],\n",
      "          [-0.5702,  0.9144,  0.6233,  ..., -0.4586, -0.1551, -0.1044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0203,  0.1264, -0.0115,  ..., -0.0265,  0.0603, -0.0511],\n",
      "          [ 0.2603, -0.5318,  0.1173,  ..., -0.2808, -0.2036,  0.0288],\n",
      "          [-0.0241, -0.5015,  0.2518,  ..., -0.8521,  0.6013, -0.0786]],\n",
      "\n",
      "         [[-0.1736, -0.1213, -0.0796,  ..., -0.2340, -0.0131, -0.0491],\n",
      "          [-0.4924, -0.4482,  0.2750,  ...,  0.6573,  0.3476,  0.1830],\n",
      "          [-0.5666, -0.8622,  0.0628,  ..., -0.5456,  0.3402, -0.2291]],\n",
      "\n",
      "         [[ 0.1252, -0.0633, -0.0357,  ..., -0.0144, -0.1005, -0.0951],\n",
      "          [-0.0367, -0.0760, -0.5294,  ...,  0.4498,  0.3809,  0.0249],\n",
      "          [ 0.4173,  0.1070,  0.0843,  ...,  0.4548,  0.5972,  0.3986]]]],\n",
      "       grad_fn=<PermuteBackward0>)), (tensor([[[[-8.7703e-01, -1.5698e-01,  3.3875e-01,  ..., -9.7211e-01,\n",
      "            3.0550e-02, -2.9612e+00],\n",
      "          [-5.2207e-01, -1.0361e+00, -2.0174e+00,  ..., -1.1214e+00,\n",
      "           -2.4264e+00,  8.3634e+00],\n",
      "          [ 2.5172e-01, -7.7829e-01, -1.6123e+00,  ..., -7.1549e-01,\n",
      "           -1.7184e+00,  1.0581e+01]],\n",
      "\n",
      "         [[ 3.7428e-01, -6.5228e-02,  4.6645e-01,  ..., -1.2610e-01,\n",
      "           -6.9532e-02, -2.2121e+00],\n",
      "          [-1.3693e+00,  2.7505e+00,  2.1944e+00,  ...,  4.8952e-01,\n",
      "            4.2529e-01,  5.2853e+00],\n",
      "          [-2.1835e+00,  2.3681e+00,  3.7522e+00,  ..., -9.0289e-01,\n",
      "            3.9722e-01,  6.9918e+00]],\n",
      "\n",
      "         [[ 1.3026e-01, -6.5164e-01, -2.1366e-01,  ...,  1.2806e-01,\n",
      "            2.6442e-01, -1.7187e-01],\n",
      "          [ 1.8182e+00,  2.6157e+00,  6.9377e-01,  ...,  7.4873e-01,\n",
      "           -1.3641e-01,  4.9642e-01],\n",
      "          [-8.0662e-01,  1.8127e+00,  7.3504e-01,  ..., -8.8217e-01,\n",
      "            1.6108e+00, -5.9608e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.8924e-01,  9.1882e-03, -3.5260e-03,  ...,  1.2416e+00,\n",
      "            6.2634e-02,  1.8025e+00],\n",
      "          [ 6.0194e-01,  2.0795e-01, -1.3536e+00,  ..., -3.0075e+00,\n",
      "           -2.5718e-01, -9.9628e-01],\n",
      "          [-6.8240e-02, -2.5822e+00,  1.7250e-01,  ..., -2.8732e+00,\n",
      "           -3.2051e-01, -2.1469e+00]],\n",
      "\n",
      "         [[-3.1835e-01, -1.2338e-01,  2.2012e-01,  ...,  2.4790e-01,\n",
      "           -3.3594e-02,  2.7430e-02],\n",
      "          [ 5.8309e-01, -5.7638e-01,  2.3104e-01,  ...,  1.1525e+00,\n",
      "           -1.1282e+00, -4.8493e-01],\n",
      "          [-2.1291e-01, -1.0578e+00, -7.4754e-01,  ...,  1.2531e+00,\n",
      "            1.5778e-01,  7.5066e-01]],\n",
      "\n",
      "         [[ 3.3853e+00,  2.1224e+00, -2.1131e+00,  ..., -2.8857e+00,\n",
      "           -3.8752e+00, -1.2515e+00],\n",
      "          [-4.8157e+00, -2.3955e+00,  5.4038e+00,  ..., -2.1631e+00,\n",
      "            1.0556e+01, -1.4467e+00],\n",
      "          [-6.0358e+00, -5.5878e+00,  5.4193e+00,  ..., -3.4152e+00,\n",
      "            1.2001e+01, -9.7357e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.5451e-03, -4.7247e-02,  3.0151e-02,  ...,  6.4475e-02,\n",
      "            2.2087e-02,  6.6163e-02],\n",
      "          [-8.7893e-03, -7.6525e-02,  7.2623e-01,  ...,  3.6287e-01,\n",
      "           -2.5065e-01, -3.5305e-01],\n",
      "          [-1.0966e-01, -5.4447e-01,  1.3028e+00,  ..., -1.3103e-01,\n",
      "            1.9099e-02, -2.1042e-01]],\n",
      "\n",
      "         [[-7.5542e-02, -1.2769e-02, -1.3052e-01,  ..., -4.6958e-02,\n",
      "            5.2543e-02, -2.2102e-02],\n",
      "          [-6.1346e-01,  1.0312e-01,  4.0711e-01,  ..., -2.2835e-01,\n",
      "            1.0663e-01,  5.9585e-01],\n",
      "          [-3.7466e-01, -1.3610e-01,  1.0684e+00,  ...,  7.2657e-01,\n",
      "            2.3536e-01,  7.9932e-01]],\n",
      "\n",
      "         [[ 6.8375e-02,  9.5091e-02,  8.9647e-02,  ...,  2.3051e-02,\n",
      "           -7.7226e-02, -2.9136e-03],\n",
      "          [ 1.7806e-01, -1.7497e-01, -1.0983e+00,  ...,  7.5840e-01,\n",
      "            1.1266e+00,  1.8899e-01],\n",
      "          [ 1.6949e-01,  6.1849e-01, -8.8561e-01,  ...,  1.5925e-01,\n",
      "            6.8011e-01,  1.3164e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8837e-03,  7.6518e-02, -7.1945e-02,  ...,  3.7841e-02,\n",
      "            3.6337e-02, -1.3033e-01],\n",
      "          [-3.0557e-01,  4.4965e-01,  5.3474e-01,  ..., -7.3308e-01,\n",
      "            6.5256e-01,  2.2196e-01],\n",
      "          [ 5.4868e-01,  6.2257e-01,  2.9661e-01,  ..., -4.3005e-01,\n",
      "            2.2770e-01, -3.7553e-01]],\n",
      "\n",
      "         [[-1.3974e-01, -4.8010e-02,  1.1130e-01,  ..., -6.1494e-02,\n",
      "            5.9508e-02, -1.9636e-02],\n",
      "          [ 1.8490e+00,  3.0431e-02, -6.8742e-01,  ..., -1.5016e-01,\n",
      "            9.7058e-01,  1.0502e+00],\n",
      "          [ 7.8361e-01,  6.6417e-02, -7.0152e-01,  ...,  7.7610e-02,\n",
      "            7.9358e-01, -1.1915e-01]],\n",
      "\n",
      "         [[-1.6090e-02, -7.0082e-03, -1.9116e-02,  ..., -2.8692e-02,\n",
      "            4.9927e-03, -3.9708e-03],\n",
      "          [ 6.9834e-01, -9.4566e-02,  1.3813e+00,  ...,  2.6430e-01,\n",
      "            2.1499e-01,  1.9605e-02],\n",
      "          [-3.4383e-02,  5.2679e-01, -1.5579e-01,  ...,  1.2735e-01,\n",
      "            2.0463e-01, -3.2943e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0266, -0.2987,  0.2359,  ...,  1.7040, -0.2036, -0.0624],\n",
      "          [-0.2260,  0.0634,  1.6674,  ..., -3.5621, -0.0779,  0.0869],\n",
      "          [ 0.8608,  0.4049, -0.0688,  ..., -4.3938, -0.2411,  2.0759]],\n",
      "\n",
      "         [[ 0.1644,  0.9792, -1.4230,  ..., -0.1359,  0.2743,  0.9200],\n",
      "          [-1.8065, -4.8041,  0.5774,  ..., -1.8375,  0.9746, -2.0507],\n",
      "          [-1.4849, -2.5278, -1.3955,  ..., -1.0774,  2.0833, -1.2246]],\n",
      "\n",
      "         [[-0.6832,  0.2461, -0.0356,  ...,  0.1957,  0.0469, -0.2879],\n",
      "          [ 3.1462, -0.5402, -1.3417,  ...,  0.2065, -0.4222, -1.5145],\n",
      "          [ 1.6996, -0.2192, -0.8980,  ...,  0.3689, -0.5965, -0.9849]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0458,  0.1237,  0.1457,  ..., -0.1083,  0.0268,  0.1678],\n",
      "          [-0.4798, -0.4531, -0.8592,  ...,  1.7757,  0.5678,  0.9070],\n",
      "          [-0.0069, -0.4577, -1.7507,  ...,  1.4605,  0.5762,  0.9845]],\n",
      "\n",
      "         [[-3.0001,  0.4016, -0.0131,  ..., -0.4612, -0.3504,  1.2311],\n",
      "          [ 4.8825,  0.6386,  0.1072,  ...,  0.2268, -0.0312, -1.9210],\n",
      "          [ 4.9529, -0.4390,  2.1437,  ..., -0.2313, -0.5828, -1.1999]],\n",
      "\n",
      "         [[-0.0065, -0.2297,  0.0207,  ..., -0.1912,  0.3517,  0.0947],\n",
      "          [ 0.1379,  0.7799,  1.8607,  ..., -1.3549, -0.1073, -1.1660],\n",
      "          [-0.0067, -0.1742,  1.7761,  ..., -0.8334, -0.4192, -1.2514]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-2.7221e-02, -3.0513e-02,  1.5327e-02,  ...,  2.3083e-03,\n",
      "           -3.8287e-02,  3.4605e-01],\n",
      "          [ 3.9876e-01,  6.0267e-01,  1.4700e+00,  ...,  3.7798e-01,\n",
      "           -7.7213e-02,  4.7471e-01],\n",
      "          [-5.7459e-02,  7.3498e-01, -7.6839e-02,  ..., -5.4373e-01,\n",
      "           -7.6723e-02,  5.8317e-01]],\n",
      "\n",
      "         [[ 7.4320e-04, -1.2304e-02,  2.4352e-02,  ..., -2.5718e-02,\n",
      "            9.5341e-03,  8.1031e-05],\n",
      "          [ 8.3789e-01,  5.4245e-01,  1.0818e+00,  ..., -6.9274e-01,\n",
      "            9.1540e-01,  8.6879e-01],\n",
      "          [-9.1864e-02,  5.9992e-01,  3.1218e-01,  ..., -4.4880e-01,\n",
      "            1.2010e+00, -3.8275e-01]],\n",
      "\n",
      "         [[-5.4730e-02,  7.6913e-03, -4.6118e-02,  ..., -3.5631e-02,\n",
      "            1.3285e-02, -8.4827e-02],\n",
      "          [ 1.0241e-01,  4.5986e-01,  8.0164e-01,  ...,  4.6153e-01,\n",
      "            4.6955e-01,  3.9161e-01],\n",
      "          [ 6.6696e-01, -8.0698e-02,  2.9961e-01,  ..., -5.9902e-01,\n",
      "            8.1882e-01, -1.7029e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3340e-01, -1.9110e-01, -6.3243e-02,  ..., -4.8256e-01,\n",
      "            2.1400e-01,  9.8984e-02],\n",
      "          [ 1.7792e+00, -4.2502e-01,  1.4064e-01,  ...,  1.3141e+00,\n",
      "           -1.3508e+00, -1.2499e+00],\n",
      "          [ 1.4492e+00, -9.7705e-01, -2.0907e-01,  ...,  1.8214e+00,\n",
      "           -4.7443e-01, -1.7283e+00]],\n",
      "\n",
      "         [[-8.2258e-02, -1.3036e-01, -4.7021e-02,  ..., -1.8429e-01,\n",
      "           -1.3480e-01,  1.3055e-01],\n",
      "          [ 4.2307e-02, -9.0837e-02, -8.1667e-02,  ..., -3.2525e-02,\n",
      "            9.1769e-01, -1.0382e+00],\n",
      "          [-3.5656e-01, -2.2010e-01, -9.0585e-01,  ...,  5.1984e-01,\n",
      "            5.9454e-01, -4.1895e-01]],\n",
      "\n",
      "         [[-3.9173e-02, -4.9848e-02,  8.7815e-02,  ...,  8.6337e-02,\n",
      "           -3.5852e-02,  1.2810e-02],\n",
      "          [ 1.4409e-01,  5.9250e-01, -9.8250e-02,  ...,  4.4417e-01,\n",
      "            8.6572e-01,  1.0504e+00],\n",
      "          [ 8.9674e-01, -8.5899e-01, -3.1211e-01,  ...,  3.3565e-01,\n",
      "           -1.4900e-01,  2.0078e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-3.3908e-01,  8.7256e-01, -1.6747e-01,  ...,  1.1273e+00,\n",
      "           -1.7125e-01,  1.3295e-01],\n",
      "          [-1.6917e-01, -4.6771e+00,  2.3620e-01,  ..., -3.7709e+00,\n",
      "            5.3759e-01,  2.0429e+00],\n",
      "          [-1.7571e+00, -4.3971e+00,  4.4556e-01,  ..., -4.4778e+00,\n",
      "           -6.7468e-01,  6.4975e-01]],\n",
      "\n",
      "         [[ 3.9112e-02,  8.6373e-01, -6.3145e-01,  ..., -2.7989e-02,\n",
      "            2.9736e-01,  1.9585e-02],\n",
      "          [-4.5871e-01,  4.0544e-01, -8.1990e-01,  ...,  2.1385e-01,\n",
      "            1.8243e-01, -8.0148e-01],\n",
      "          [ 2.1735e-01, -1.0100e+00,  7.5429e-02,  ..., -7.1476e-01,\n",
      "            5.8830e-01,  5.6416e-01]],\n",
      "\n",
      "         [[-3.1253e-01,  1.3100e-01, -9.8441e-01,  ..., -3.4806e-01,\n",
      "           -5.7390e-02, -1.4779e-01],\n",
      "          [-6.8702e-01,  6.4847e-01,  3.3417e+00,  ...,  1.5018e+00,\n",
      "           -1.9523e-01, -9.2132e-01],\n",
      "          [-8.0346e-01,  8.1773e-01,  3.3439e+00,  ...,  1.6296e+00,\n",
      "           -2.5054e-02, -9.2615e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7047e-01,  6.8842e-02, -7.2620e-02,  ..., -4.0570e-02,\n",
      "            2.2521e-01,  4.3733e-03],\n",
      "          [-4.4866e-02, -2.9037e-01, -1.3400e+00,  ..., -9.8825e-01,\n",
      "           -1.1635e-01, -1.9321e+00],\n",
      "          [-3.2209e+00, -1.5065e+00, -3.4319e-01,  ...,  1.5834e+00,\n",
      "            7.8861e-01, -2.2485e+00]],\n",
      "\n",
      "         [[ 1.9911e-01,  6.8349e-02,  3.0913e-01,  ...,  4.1207e-01,\n",
      "            2.9999e-02,  2.1895e-01],\n",
      "          [ 3.2056e-01,  9.0291e-01,  9.4325e-01,  ..., -3.6162e-01,\n",
      "            3.9871e-01,  6.5507e-01],\n",
      "          [ 3.0717e-01,  5.8557e-01, -1.8444e-01,  ..., -1.9561e+00,\n",
      "            5.6182e-01,  9.9491e-01]],\n",
      "\n",
      "         [[-3.0157e+00,  5.3949e-01,  5.5724e-01,  ..., -9.1519e-01,\n",
      "            3.2315e-01,  1.9583e-01],\n",
      "          [ 7.0446e+00, -2.1651e+00, -3.2884e-01,  ...,  2.8114e+00,\n",
      "            9.1413e-01,  1.0158e+00],\n",
      "          [ 7.7961e+00, -2.3373e+00, -3.5468e-02,  ...,  2.1246e+00,\n",
      "            8.2858e-01,  3.3505e-03]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 4.5983e-02, -4.4784e-02,  1.3706e-02,  ..., -7.4682e-02,\n",
      "           -7.3998e-04, -8.7975e-02],\n",
      "          [-1.5158e-01, -4.8461e-02, -9.8416e-02,  ..., -3.5967e-01,\n",
      "            6.5078e-01, -7.5144e-01],\n",
      "          [-1.8767e-01, -4.0522e-01,  4.1696e-01,  ..., -1.1364e+00,\n",
      "            4.9276e-01,  3.3071e-01]],\n",
      "\n",
      "         [[ 6.8963e-02,  2.2769e-02, -2.2675e-02,  ..., -3.4925e-02,\n",
      "            1.2893e-02, -8.6573e-03],\n",
      "          [ 1.3532e+00,  1.2331e+00,  7.3712e-01,  ...,  1.1518e-01,\n",
      "           -6.3342e-03,  2.9508e-01],\n",
      "          [ 1.5249e+00,  1.5522e+00,  1.9085e-01,  ...,  1.7119e-01,\n",
      "            7.2426e-01,  2.2903e-01]],\n",
      "\n",
      "         [[ 8.2035e-02,  1.6159e-02,  8.6496e-03,  ...,  1.5957e-02,\n",
      "           -6.5116e-02, -6.5114e-02],\n",
      "          [-1.7365e-01, -8.9208e-02, -3.9057e-01,  ..., -7.5146e-01,\n",
      "            1.4309e+00,  3.4480e-01],\n",
      "          [ 7.3663e-01, -8.6358e-02, -1.8769e-01,  ..., -1.7822e+00,\n",
      "           -6.8067e-02, -2.3995e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.2665e-03,  2.8322e-02,  2.9798e-02,  ..., -7.9864e-02,\n",
      "           -1.2316e-02,  2.8138e-02],\n",
      "          [-1.0211e+00, -7.3721e-02,  1.6045e-01,  ...,  6.8689e-01,\n",
      "            5.8972e-01,  1.3542e-01],\n",
      "          [-9.7730e-01, -4.3840e-01,  2.7323e-01,  ...,  4.8912e-01,\n",
      "           -1.5197e-01, -1.0947e+00]],\n",
      "\n",
      "         [[ 3.8820e-02, -5.3460e-03,  3.5535e-02,  ...,  2.4933e-02,\n",
      "           -2.6117e-03,  4.4950e-03],\n",
      "          [ 1.1301e+00, -5.3400e-01,  5.2494e-01,  ...,  1.1671e+00,\n",
      "           -8.8095e-01, -7.8981e-01],\n",
      "          [ 5.6378e-01, -6.8983e-01, -3.8785e-01,  ...,  1.3740e+00,\n",
      "           -9.8549e-01, -9.8376e-01]],\n",
      "\n",
      "         [[ 6.9299e-02, -1.8554e-01, -5.7382e-02,  ..., -3.6573e-02,\n",
      "            1.8694e-01, -5.3460e-02],\n",
      "          [-5.3510e-02, -5.4322e-01, -8.4122e-01,  ..., -5.1779e-01,\n",
      "           -1.1805e+00,  1.3371e-02],\n",
      "          [ 4.9183e-01,  4.5194e-01, -1.0471e+00,  ...,  3.7614e-01,\n",
      "           -7.1822e-02,  2.5536e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0422, -0.2508, -0.1327,  ...,  0.6354,  0.7285, -0.2898],\n",
      "          [-3.7335, -3.1803, -0.5051,  ..., -0.1036, -3.7673,  1.7087],\n",
      "          [-4.0487, -3.6223,  1.0210,  ...,  0.9449, -3.0742,  0.6590]],\n",
      "\n",
      "         [[-0.1466, -0.0552,  0.1675,  ..., -0.0544, -0.8915, -0.2030],\n",
      "          [-0.3108, -0.3040, -1.1040,  ...,  0.1752, -0.0070,  0.3625],\n",
      "          [ 0.9399,  0.8943, -0.8171,  ..., -1.7868,  0.6262,  0.6576]],\n",
      "\n",
      "         [[ 0.1987,  0.2998,  1.1332,  ..., -0.4733,  0.4336, -0.4903],\n",
      "          [ 0.6300, -1.5629, -2.0427,  ..., -1.1909, -1.0048,  0.2611],\n",
      "          [ 1.4585, -1.1964, -1.8696,  ..., -0.8846, -0.4198, -0.8467]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1526,  0.0727, -0.2390,  ...,  0.0131,  0.1477,  0.0169],\n",
      "          [-1.3393, -0.4749, -0.2910,  ...,  1.7193, -1.1155,  0.6555],\n",
      "          [-1.6415,  1.0012,  0.1350,  ...,  1.7357, -1.0199,  0.1981]],\n",
      "\n",
      "         [[-0.3542, -2.1987,  0.1137,  ..., -0.0864, -0.0473,  0.9172],\n",
      "          [ 2.1556,  1.6787, -0.6426,  ...,  0.7362, -0.4434, -0.8795],\n",
      "          [ 1.0748,  3.7835, -0.7439,  ...,  1.4590,  0.8159, -0.1913]],\n",
      "\n",
      "         [[ 0.3711,  0.0609, -0.1321,  ...,  0.6386,  0.1386,  0.2467],\n",
      "          [ 0.1295, -0.7718, -0.5181,  ...,  0.3714, -1.0199,  0.1716],\n",
      "          [-1.1952, -2.4470,  0.2837,  ...,  0.3156, -0.2708, -0.1452]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-3.2704e-02,  4.1300e-02, -5.7466e-02,  ..., -2.0020e-02,\n",
      "            1.0783e-02,  2.2392e-02],\n",
      "          [-3.7934e-01, -4.7512e-01, -6.8974e-01,  ..., -3.2730e-02,\n",
      "           -4.5526e-01,  3.8799e-02],\n",
      "          [-4.2813e-01, -8.9832e-01, -3.5044e-01,  ..., -8.0293e-03,\n",
      "           -1.4125e+00,  3.0705e-01]],\n",
      "\n",
      "         [[ 2.0737e-02, -2.5582e-02,  2.9250e-02,  ...,  3.1316e-02,\n",
      "           -4.1396e-02,  1.7303e-02],\n",
      "          [ 5.8320e-01,  4.6427e-01, -1.9233e-01,  ..., -7.3958e-02,\n",
      "            4.3195e-01,  1.3205e-01],\n",
      "          [ 8.9463e-01, -7.5869e-02, -4.6746e-01,  ..., -3.3260e-01,\n",
      "           -2.3234e-01,  7.9537e-01]],\n",
      "\n",
      "         [[ 3.5919e-02, -2.8122e-02,  4.4534e-02,  ...,  3.2025e-02,\n",
      "            3.0503e-03,  1.0178e-02],\n",
      "          [ 6.2747e-01, -1.4712e+00, -7.7325e-01,  ...,  2.5261e+00,\n",
      "            3.9913e-01, -2.0472e+00],\n",
      "          [-4.3348e-01, -8.5008e-01, -9.3632e-01,  ..., -1.0324e+00,\n",
      "           -7.5543e-02, -8.3242e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8452e-01,  9.5417e-02,  4.6628e-02,  ...,  3.9127e-02,\n",
      "            3.7104e-02, -1.2257e-01],\n",
      "          [-6.6315e-01,  1.2922e+00,  4.9658e-01,  ..., -4.0196e-02,\n",
      "            5.9267e-01, -1.4106e-01],\n",
      "          [-1.1044e+00, -2.4121e-01,  4.1160e-01,  ..., -4.6429e-02,\n",
      "           -4.8386e-01, -6.4485e-01]],\n",
      "\n",
      "         [[-5.8360e-01, -4.3886e-03,  4.7745e-02,  ..., -1.1464e-02,\n",
      "            2.0897e-02, -3.6598e-04],\n",
      "          [-6.5028e-01,  1.3769e+00,  1.7303e+00,  ...,  2.2482e+00,\n",
      "            4.0286e-01,  1.1731e+00],\n",
      "          [-1.0479e+00, -3.0895e-01, -8.9157e-02,  ...,  1.3688e+00,\n",
      "           -7.4806e-01,  1.2785e+00]],\n",
      "\n",
      "         [[ 1.2303e-02,  8.9485e-02, -5.3582e-02,  ...,  7.3090e-02,\n",
      "            2.3558e-02, -3.8593e-02],\n",
      "          [ 6.4961e-01,  1.7409e+00,  6.9627e-01,  ..., -3.6254e-02,\n",
      "            2.6160e-01, -3.8927e-01],\n",
      "          [-1.8245e-01,  2.1148e+00, -3.8478e-01,  ..., -8.1045e-01,\n",
      "            3.4101e-01, -9.3509e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-2.6364e-02, -2.3388e+00,  1.8140e-01,  ..., -2.2851e-01,\n",
      "           -2.0508e-01,  7.3237e-02],\n",
      "          [ 9.1943e-01,  3.9038e+00,  1.9964e-01,  ..., -3.4466e-01,\n",
      "            4.5787e-01,  4.6802e-01],\n",
      "          [ 2.6542e-02,  4.2073e+00,  1.8387e+00,  ..., -1.3017e+00,\n",
      "            1.0871e-01,  3.5718e-01]],\n",
      "\n",
      "         [[-8.1900e-01,  2.1007e-01,  4.7354e-01,  ..., -5.1898e-01,\n",
      "            1.0763e+00,  1.1317e+00],\n",
      "          [ 5.1593e-01,  2.0259e-01,  1.0728e-01,  ..., -1.0248e+00,\n",
      "            1.4249e+00, -1.7698e+00],\n",
      "          [-1.4279e+00,  3.9753e-01,  9.4044e-01,  ...,  5.3461e-01,\n",
      "            7.9845e-01, -2.0728e+00]],\n",
      "\n",
      "         [[-8.5709e-01,  4.6948e-01,  3.1137e-02,  ...,  5.0459e-01,\n",
      "           -2.2679e-01,  1.1609e+00],\n",
      "          [ 1.0999e+00,  5.7358e-01, -1.5519e-03,  ...,  1.1821e+00,\n",
      "           -2.6563e-01,  1.2500e-01],\n",
      "          [ 2.0791e+00, -2.6543e-01,  3.3467e-02,  ...,  2.2969e+00,\n",
      "            7.0491e-01,  1.6618e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9409e-01, -1.2283e-01,  1.4279e-01,  ...,  1.9383e-01,\n",
      "            1.7308e+00, -2.8647e+00],\n",
      "          [-3.2932e-01,  7.4039e-01,  6.1064e-02,  ...,  3.9045e-01,\n",
      "           -3.7998e+00,  4.9879e+00],\n",
      "          [-7.4057e-01, -1.4239e-01,  3.2645e-01,  ...,  4.8435e-01,\n",
      "           -3.5779e+00,  5.7455e+00]],\n",
      "\n",
      "         [[ 1.9142e-01,  3.7523e-01,  2.1773e-01,  ..., -2.1369e-01,\n",
      "            1.7324e-02, -1.4348e-01],\n",
      "          [-7.0662e-01, -5.8611e-01,  1.5639e-01,  ...,  1.2616e+00,\n",
      "           -1.3992e-01,  4.9165e-01],\n",
      "          [-5.7968e-01,  5.7127e-01, -9.4050e-01,  ...,  1.4159e+00,\n",
      "           -3.4126e-01,  2.4666e-01]],\n",
      "\n",
      "         [[ 3.6516e-01,  1.2126e-01,  6.2172e-01,  ...,  5.1247e-01,\n",
      "            5.7641e-01, -3.2638e-01],\n",
      "          [ 3.1706e-01, -1.1233e+00,  6.0494e-01,  ..., -1.3075e+00,\n",
      "           -2.7268e+00,  4.6166e-01],\n",
      "          [-5.1901e-01,  8.8207e-01,  1.2787e-01,  ..., -1.3744e+00,\n",
      "           -3.1466e+00,  3.5762e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.0609, -0.0081, -0.0167,  ...,  0.1210, -0.0771, -0.0416],\n",
      "          [-0.3227, -0.0045, -1.0303,  ..., -1.1382,  0.7274,  0.0072],\n",
      "          [-1.0812,  0.1388, -1.6567,  ..., -1.1944,  0.9394,  0.2467]],\n",
      "\n",
      "         [[ 0.0109,  0.0480,  0.0413,  ...,  0.0049,  0.0037, -0.0132],\n",
      "          [ 0.8589, -0.2053, -1.2311,  ..., -0.1288, -0.3660, -0.7660],\n",
      "          [ 0.5202, -0.6736,  0.6070,  ..., -0.2767, -1.1892, -0.4967]],\n",
      "\n",
      "         [[ 0.0505, -0.0471,  0.0641,  ...,  0.0446, -0.0705, -0.0591],\n",
      "          [-0.6367, -0.8215, -0.8183,  ..., -0.4746,  1.0494, -0.5067],\n",
      "          [-0.8163, -1.0031, -0.1412,  ...,  0.8973,  1.2382,  1.6296]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0967, -0.0534,  0.0367,  ..., -0.0908,  0.0321,  0.0069],\n",
      "          [ 0.0243, -1.1389,  0.2142,  ..., -0.6391,  0.8593, -0.0849],\n",
      "          [ 0.2091, -1.7048,  2.8748,  ..., -0.7132,  0.1024, -1.1891]],\n",
      "\n",
      "         [[ 0.1647, -0.0692,  0.1265,  ...,  0.0718,  0.0247, -0.1250],\n",
      "          [-0.6575, -1.0379, -0.2624,  ..., -1.5211, -0.1557, -0.4203],\n",
      "          [ 0.6075, -0.7917, -1.0712,  ...,  0.1816,  0.5527,  0.4613]],\n",
      "\n",
      "         [[ 0.2057, -0.0410, -0.0650,  ...,  0.0265,  0.0480,  0.0267],\n",
      "          [ 0.3731,  0.0467, -0.0799,  ...,  0.2745, -0.0526, -1.4381],\n",
      "          [ 0.1397,  0.4439,  0.5892,  ...,  0.5546, -0.8746, -1.2705]]]],\n",
      "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0302, -0.2538, -0.4319,  ...,  0.3064,  0.3234,  0.3837],\n",
      "          [-0.9722, -0.0683, -2.3672,  ..., -0.9778, -0.5862, -1.0055],\n",
      "          [-0.1419,  1.5504, -2.6688,  ..., -0.6188,  0.1250, -0.0934]],\n",
      "\n",
      "         [[-0.2801,  0.1649,  0.1158,  ...,  0.0354, -1.1552, -0.1514],\n",
      "          [ 1.1021,  0.9597, -0.8445,  ...,  0.6094, -0.3904,  0.0425],\n",
      "          [ 3.1594,  1.2577, -0.3191,  ...,  1.4046, -2.3153,  1.3728]],\n",
      "\n",
      "         [[-1.2204, -0.0929,  0.5568,  ..., -0.6611,  0.4621, -0.2648],\n",
      "          [ 1.6340,  0.5047,  0.6792,  ..., -0.0676, -1.1558,  1.0838],\n",
      "          [ 0.5096,  1.0767,  1.7081,  ..., -0.4130, -0.8518,  1.8206]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7928, -0.8873, -0.3956,  ..., -1.0324, -0.4183,  0.5020],\n",
      "          [-0.4287, -1.0954, -0.4679,  ...,  1.5172,  0.9249, -0.8194],\n",
      "          [-0.5842, -0.6158, -1.8915,  ...,  1.0584,  1.7749, -0.0122]],\n",
      "\n",
      "         [[-0.9106,  2.5597,  0.2983,  ...,  0.3390,  1.9621, -0.5324],\n",
      "          [ 0.4878, -2.6981,  0.1724,  ..., -0.3462, -3.5434,  0.8603],\n",
      "          [-0.7906, -2.3804,  1.5356,  ..., -1.1216, -3.1855,  0.2825]],\n",
      "\n",
      "         [[-2.0188, -0.3637, -1.1280,  ..., -0.3927,  0.0609,  0.2440],\n",
      "          [ 0.5410, -0.7857,  0.3808,  ...,  1.0862,  0.1742,  0.9014],\n",
      "          [ 1.9448, -0.0162, -0.2241,  ..., -1.3317,  1.4815, -0.1541]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-4.0149e-02, -8.0115e-02,  3.6354e-02,  ...,  1.1824e-01,\n",
      "           -2.9524e-02,  2.0266e-02],\n",
      "          [-6.0421e-01,  2.2656e-01,  1.2498e-01,  ..., -1.2188e+00,\n",
      "           -2.7717e-01, -1.4065e+00],\n",
      "          [ 1.5298e-02, -1.6624e+00,  1.1874e+00,  ...,  7.0220e-02,\n",
      "           -2.3255e-01, -7.3476e-01]],\n",
      "\n",
      "         [[ 2.3250e-02,  1.5313e-02, -4.0914e-02,  ...,  2.9765e-02,\n",
      "            1.9571e-03,  4.4732e-02],\n",
      "          [ 8.6186e-02, -1.6052e+00,  2.2301e-01,  ..., -6.0161e-01,\n",
      "            3.5788e-01,  3.4378e-01],\n",
      "          [ 1.3231e+00, -2.0112e-01, -2.8871e-01,  ...,  2.2463e+00,\n",
      "           -8.4205e-01,  1.3744e+00]],\n",
      "\n",
      "         [[ 3.8498e-02,  2.7246e-02, -8.8599e-02,  ...,  1.2417e-02,\n",
      "           -2.3073e-02,  7.5459e-03],\n",
      "          [-5.0341e-02, -6.8508e-01, -4.1764e-01,  ...,  8.5075e-02,\n",
      "           -3.1446e-01, -8.6349e-01],\n",
      "          [-5.4208e-01,  3.3410e-01,  1.6957e+00,  ...,  8.9077e-01,\n",
      "           -1.0071e+00, -1.1072e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4051e-02,  1.4939e-02, -1.1746e-02,  ..., -2.2520e-02,\n",
      "           -1.3826e-02,  2.9657e-02],\n",
      "          [ 2.5206e+00, -1.0295e-01,  8.6813e-01,  ..., -5.2228e-01,\n",
      "            8.8117e-01, -1.0967e+00],\n",
      "          [ 6.2327e-01,  3.2369e-01,  1.8336e-01,  ..., -1.4948e+00,\n",
      "           -8.0949e-02,  6.8240e-01]],\n",
      "\n",
      "         [[-7.6975e-02, -3.6957e-02,  2.7709e-02,  ...,  1.0812e-02,\n",
      "           -6.0294e-02, -9.5978e-02],\n",
      "          [-2.4084e-01, -4.4508e-01,  1.3782e+00,  ...,  6.6155e-01,\n",
      "           -6.5512e-01, -3.4212e-01],\n",
      "          [-1.1455e+00, -4.2709e-01, -5.1476e-01,  ..., -4.5379e-02,\n",
      "           -1.3732e+00,  3.3834e-01]],\n",
      "\n",
      "         [[-8.4658e-03,  3.2352e-02, -4.3181e-02,  ..., -1.8599e-02,\n",
      "            1.5865e-02,  1.4038e-02],\n",
      "          [-1.0688e+00,  5.1150e-01,  4.6432e-01,  ...,  2.6218e+00,\n",
      "            4.3147e-01, -2.5632e-01],\n",
      "          [ 3.8763e-02, -4.0703e-01,  1.2107e+00,  ...,  1.5708e+00,\n",
      "           -1.3612e+00,  5.3368e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5213,  0.4844, -0.8613,  ..., -1.0203, -1.3334,  0.2146],\n",
      "          [ 0.8275,  0.7202,  0.0887,  ...,  1.2508, -0.1565, -1.0561],\n",
      "          [ 0.9601, -0.1651,  0.1825,  ...,  1.6772, -1.2772, -1.0362]],\n",
      "\n",
      "         [[ 0.8382, -2.0604,  0.1540,  ...,  0.2357, -2.4577, -0.4528],\n",
      "          [ 2.3938,  1.2861, -0.1475,  ...,  0.6070,  0.7725, -0.9543],\n",
      "          [ 0.9790,  2.2406,  0.1288,  ...,  0.3160,  0.8776, -0.7339]],\n",
      "\n",
      "         [[ 1.0092,  0.3949, -0.1669,  ..., -0.8061, -1.3972, -0.3597],\n",
      "          [-0.3789, -0.7504,  0.2506,  ..., -0.6285, -0.6698,  0.2380],\n",
      "          [-0.8148, -0.6553,  0.7710,  ..., -0.1502, -1.5709,  0.6907]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2134, -0.5879,  0.4856,  ..., -0.6650,  1.0787,  0.2896],\n",
      "          [-1.4638,  0.5061, -1.9712,  ..., -2.1186, -1.5694,  1.2018],\n",
      "          [-1.6572,  1.7763, -2.4928,  ..., -1.1972, -1.9295,  0.5836]],\n",
      "\n",
      "         [[ 0.2620,  0.5383,  0.5528,  ...,  0.7387,  0.0602,  0.8380],\n",
      "          [ 0.0597,  0.7884,  0.6142,  ...,  1.2837, -0.2903,  0.5242],\n",
      "          [-0.4306,  0.3731,  0.3008,  ...,  0.5311,  0.0906,  0.8987]],\n",
      "\n",
      "         [[-0.7142,  0.3192, -1.5675,  ..., -0.3943,  0.2318, -1.3325],\n",
      "          [ 0.1654, -0.1526,  1.3245,  ...,  1.0666,  0.2895, -0.0332],\n",
      "          [-0.2767, -0.6815,  1.2153,  ...,  0.1852,  0.2212, -0.2537]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.6322e-02,  3.8011e-02, -7.6123e-02,  ...,  7.1516e-02,\n",
      "           -4.4584e-02, -8.4583e-02],\n",
      "          [ 9.3163e-01,  1.3517e+00,  9.6631e-02,  ..., -1.4388e+00,\n",
      "            1.0957e+00, -5.4389e-01],\n",
      "          [ 1.2971e+00,  9.8367e-01, -1.4761e+00,  ..., -3.9591e-01,\n",
      "            2.6882e+00, -2.2715e+00]],\n",
      "\n",
      "         [[ 5.2631e-02, -1.0291e-03,  3.1466e-02,  ..., -3.9356e-02,\n",
      "           -2.4391e-02, -1.6028e-02],\n",
      "          [-8.5729e-01, -7.7357e-01, -1.6951e+00,  ...,  1.0828e+00,\n",
      "            9.6435e-01,  3.1165e-01],\n",
      "          [ 1.0858e-01,  3.1846e-01, -1.3145e+00,  ..., -3.3012e-01,\n",
      "            2.5732e-02,  6.4859e-01]],\n",
      "\n",
      "         [[-4.4263e-03,  2.7861e-02, -4.6152e-02,  ..., -1.2522e-02,\n",
      "            2.4193e-02,  6.7562e-02],\n",
      "          [ 1.5651e+00, -2.7014e+00, -2.8728e-01,  ...,  1.5415e+00,\n",
      "            2.0986e-01,  1.4569e+00],\n",
      "          [-6.8185e-01, -8.8485e-01,  1.2992e+00,  ...,  4.5138e-02,\n",
      "            1.7742e-01,  6.0503e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.1136e-02,  2.7894e-02, -1.5999e-02,  ..., -3.9158e-02,\n",
      "           -8.5000e-03,  1.5245e-02],\n",
      "          [ 8.8529e-01, -3.6419e-02,  3.4305e-02,  ...,  1.0617e-01,\n",
      "           -1.0280e-01,  8.4350e-01],\n",
      "          [ 1.4757e-01, -2.9460e-01, -1.6871e+00,  ...,  1.0639e-02,\n",
      "           -1.0931e+00, -3.6619e-01]],\n",
      "\n",
      "         [[ 6.8468e-02,  1.3984e-02,  7.4126e-02,  ...,  4.4410e-02,\n",
      "            5.5519e-02,  3.8163e-02],\n",
      "          [ 1.5120e+00, -1.8734e+00,  1.2200e+00,  ...,  1.8619e+00,\n",
      "            3.2587e-01,  4.0816e-01],\n",
      "          [ 1.1303e-02,  3.9365e-01,  3.3558e+00,  ...,  1.3415e+00,\n",
      "            1.8727e-01,  1.3670e-01]],\n",
      "\n",
      "         [[-1.0749e-01,  2.7079e-02, -5.7126e-02,  ..., -1.0529e-01,\n",
      "            7.2973e-02, -1.1574e-02],\n",
      "          [-2.4257e-01,  7.2043e-01,  6.5882e-01,  ..., -6.3618e-01,\n",
      "            1.3263e+00, -3.1769e-01],\n",
      "          [ 7.0326e-02,  1.2849e+00,  5.6563e-01,  ..., -9.6328e-01,\n",
      "            1.4866e+00,  5.9157e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.6944, -0.3010, -0.2875,  ...,  0.1682,  0.3199, -0.5173],\n",
      "          [-0.7439, -0.6253, -1.0639,  ...,  0.6156, -1.1364, -0.1848],\n",
      "          [-0.8450,  0.0307,  0.0745,  ...,  0.9592, -0.6858,  0.6032]],\n",
      "\n",
      "         [[ 0.1170, -0.0812,  2.2956,  ...,  0.2388,  0.0711, -0.2008],\n",
      "          [ 1.1998, -0.6757, -0.5277,  ...,  0.2370,  0.5188,  0.6353],\n",
      "          [ 0.8645, -0.5631, -0.3045,  ...,  0.6318,  0.7730,  0.5473]],\n",
      "\n",
      "         [[-0.1973,  1.0540,  0.4509,  ..., -0.5271,  0.3178, -0.1056],\n",
      "          [-0.2999,  0.8185,  0.3525,  ...,  1.3822,  0.3487, -0.9810],\n",
      "          [-1.2547, -0.1946,  0.9342,  ...,  1.3691,  0.4892, -1.8890]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5508,  0.9897, -0.8529,  ..., -0.7107,  0.6843,  0.8529],\n",
      "          [ 0.4473,  0.7905, -1.0300,  ..., -1.1152, -0.2716, -0.5382],\n",
      "          [ 0.2012,  1.3928,  0.0672,  ..., -1.9587, -1.5108, -0.3405]],\n",
      "\n",
      "         [[-0.4046,  0.3744,  0.3415,  ...,  0.6997,  0.0181, -0.0903],\n",
      "          [-0.3283, -0.0898, -0.3434,  ..., -0.2300, -0.4418, -0.8055],\n",
      "          [-1.0908,  0.7655,  0.8097,  ..., -0.3710, -1.5594, -1.2446]],\n",
      "\n",
      "         [[-0.7250, -0.0313,  0.4862,  ..., -0.0822,  0.0430, -0.0688],\n",
      "          [-0.5425, -1.6332,  1.4566,  ...,  1.1406, -0.0950,  2.0363],\n",
      "          [-1.2490, -1.8330,  2.2674,  ...,  1.0855, -1.0947,  1.4252]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 5.6022e-02, -1.2348e-01, -1.7584e-01,  ..., -3.1333e-01,\n",
      "            2.5030e-01, -1.4776e-01],\n",
      "          [-1.8265e+00,  1.1500e-01,  9.4245e-01,  ...,  1.3880e+00,\n",
      "           -3.2597e-01,  1.6253e+00],\n",
      "          [-1.0732e+00, -1.0394e+00,  3.5278e-01,  ...,  1.2416e+00,\n",
      "           -1.6986e+00,  1.3403e+00]],\n",
      "\n",
      "         [[ 1.0174e-01, -1.4738e-02,  2.6016e-02,  ..., -1.8769e-02,\n",
      "           -1.0673e-01,  1.6367e-01],\n",
      "          [ 6.9146e-01, -3.2193e-01, -3.6843e-01,  ..., -1.1148e+00,\n",
      "           -5.8268e-01, -4.3033e-01],\n",
      "          [ 8.6693e-01,  1.5000e+00,  1.3468e+00,  ..., -3.5565e-01,\n",
      "           -9.5769e-01, -5.2765e-02]],\n",
      "\n",
      "         [[ 9.0313e-03,  8.7008e-03, -4.7023e-02,  ...,  4.6042e-02,\n",
      "            1.3024e-02,  2.2466e-02],\n",
      "          [ 1.3610e+00,  3.0184e-01,  4.2720e-02,  ...,  1.4966e+00,\n",
      "            3.7859e-01,  1.5265e+00],\n",
      "          [-2.7711e-01,  4.1025e-01, -1.5459e+00,  ...,  7.1072e-01,\n",
      "            2.3472e+00,  8.3725e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4919e-03, -1.9926e-02,  8.3196e-02,  ...,  4.9395e-02,\n",
      "           -7.0040e-03,  3.0570e-02],\n",
      "          [-9.7443e-01,  1.0781e+00, -1.3359e+00,  ...,  8.1234e-01,\n",
      "            1.1833e+00,  1.3833e+00],\n",
      "          [ 1.2942e+00,  5.6075e-01, -2.8122e+00,  ...,  9.3041e-01,\n",
      "            7.3071e-01, -3.0994e-02]],\n",
      "\n",
      "         [[-2.0459e-01, -6.4667e-02,  7.9892e-02,  ..., -6.7050e-02,\n",
      "            4.7268e-02, -9.3680e-02],\n",
      "          [-1.6848e-01,  7.4851e-01,  5.6363e-01,  ..., -9.2159e-01,\n",
      "            1.6433e+00, -1.0587e+00],\n",
      "          [ 4.7029e-01,  1.4451e+00,  1.7004e-01,  ..., -1.4908e-01,\n",
      "            6.7224e-01,  8.3964e-03]],\n",
      "\n",
      "         [[ 1.2859e-01, -1.1749e-01,  1.3468e-01,  ..., -9.2371e-02,\n",
      "            1.1439e-03, -1.8096e-01],\n",
      "          [-3.4846e-01, -4.1257e-01, -1.0317e+00,  ...,  1.3214e+00,\n",
      "            9.5279e-01,  1.3955e+00],\n",
      "          [-8.7369e-01,  1.2184e+00, -8.9642e-01,  ...,  1.9668e+00,\n",
      "            1.8389e-01,  3.9906e-01]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "gpt = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "tokens = torch.tensor([1, 2, 3])\n",
    "print(gpt(  tokens  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(text_data):\n",
    "    # Convert your text data to token IDs\n",
    "    # This is just an example - adjust based on your tokenizer\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": text_data,\n",
    "    })\n",
    "\n",
    "train_dataset = prepare_dataset(data)\n",
    "# eval_dataset = prepare_dataset(your_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99fc9528ed64e49a0a984db3d63a94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mgpt,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     18\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# eval_dataset=eval_dataset,\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/its530_py38/lib/python3.8/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/its530_py38/lib/python3.8/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/its530_py38/lib/python3.8/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/its530_py38/lib/python3.8/site-packages/transformers/trainer.py:2943\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2943\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2945\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2946\u001b[0m         )\n\u001b[1;32m   2947\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2948\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    # per_device_eval_batch_size=batch_size,\n",
    "    # eval_steps=eval_interval,\n",
    "    save_steps=eval_interval,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=learning_rate,\n",
    "    # fp16=True,  # if you want to use mixed precision training\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=eval_interval,\n",
    ")\n",
    "\n",
    "# Wrap the model\n",
    "config = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model_for_trainer = GPTModelForTrainer(config, model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_trainer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its530_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
