{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<module 'torch.backends.mps' from 'C:\\\\Users\\\\joeya\\\\anaconda3\\\\envs\\\\py38-ITS520-Project\\\\lib\\\\site-packages\\\\torch\\\\backends\\\\mps\\\\__init__.py'>\n",
      "devices: 1\n",
      "device:  NVIDIA GeForce RTX 3080\n",
      "device0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080', major=8, minor=6, total_memory=10239MB, multi_processor_count=68)\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    # get number of cuda devices\n",
    "    print(f\"devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"device:  {torch.cuda.get_device_name()}\")\n",
    "    print(f\"device0: {torch.cuda.get_device_properties(0)}\")\n",
    "    print(f\"{torch.cuda.memory_summary()}\")\n",
    "elif torch.backends.mps is not None:\n",
    "    device = torch.device('mps')\n",
    "    print(f\"{torch.mps.current_allocated_memory()}\")\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    # print a warning that cpu is being used\n",
    "    print(\"Warning: Running on CPU. This will be slow.\")\n",
    "print(f\"{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, block_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
    "\n",
    "        tril_def = torch.tril( torch.ones(block_size, block_size) )  ## [40, 40]\n",
    "        \n",
    "        self.register_buffer(\n",
    "                  'tril', \n",
    "                  tril_def\n",
    "               )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, E = x.shape   ## [batch_size, 40, 512]\n",
    "        \n",
    "        k = self.key(   x )            ## k = (B, T, 64)\n",
    "        q = self.query( x )            ## q = (B, T, 64)\n",
    "\n",
    "        E2 = 64     ## I think this is 64 and not 512\n",
    "        ## (B, T, E) @ (B, E, T)  -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5        \n",
    "        \n",
    "        wei = wei.masked_fill(\n",
    "                      self.tril[:T, :T] == 0, \n",
    "                      float('-inf')\n",
    "        )   \n",
    "        \n",
    "        ## (B, T, T)\n",
    "        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)\n",
    "        wei = self.dropout(   wei   )\n",
    "        \n",
    "        ## perform weighted aggregation of values\n",
    "        \n",
    "        v   = self.value(  x  )   ## x = (B, 40, E)\n",
    "        out = wei @ v             ## (B, T, T) @ (B, T, 64) -> (B, T, 64)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, dropout):         ## 512\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),      ## [512, 4*512]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),      ## [4*512, 512]\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, head_size, block_size, n_embd, dropout):    ## (8, 64)\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(  [ Head(head_size, block_size, n_embd, dropout) for _ in range(n_head) ] )\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)   ## 512, 512\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat(   [ h(x) for h in self.heads ], dim = -1   )\n",
    "        out = self.proj(  out   )\n",
    "        out = self.dropout(   out   )\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_head, block_size, n_embd, dropout):     ## (512, 8)\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head        ## 64\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size, block_size, n_embd, dropout)\n",
    "        self.ffwd = FeedForward( n_embd, dropout)    ## 512\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(     self.ln1(x)      )\n",
    "        x = x + self.ffwd(   self.ln2(x)      )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,vocab_size, n_embd, block_size, n_layer, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   ## [65, 512]\n",
    "        self.pos_emb_table = nn.Embedding(block_size, n_embd)     ## [block, 512]\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[ Block(n_head, block_size, n_embd, dropout) for _ in range(n_layer) ]\n",
    "        )\n",
    "        \n",
    "        self.ln_f    = nn.LayerNorm(  n_embd    )        \n",
    "        self.lm_ffw_head = nn.Linear(n_embd, vocab_size)  ## [512, 65] # FFW Layer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape     ## (Batch, 40)\n",
    "        ## ids and targets are both (B, T) tensors of integers\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))  \n",
    "        \n",
    "        x = tok_emb + pos_emb    ## [B, T, E] or [64, 40, 512]\n",
    "\n",
    "        ## This is the architecture\n",
    "        x = self.blocks(  x  )   ## (B, T, E)        \n",
    "        x = self.ln_f(    x  )   ## (B, T, E)   ## norm\n",
    "        logits = self.lm_ffw_head(x)         ## [B, 40, 65] \n",
    "        \n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, E  = logits.shape\n",
    "        #     logits  = logits.view( B*T, E)\n",
    "        #     targets = targets.view(B*T)\n",
    "        #     loss    = F.cross_entropy(logits, targets)\n",
    "        return logits#, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            ## crop idx to the last block_size tokens\n",
    "            # idx_cond, _loss\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)    ## ## get preds\n",
    "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
    "            probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
    "            idx = torch.cat(  (idx, idx_next), dim=1  )   ## (B, T+1) append sample to running sequence\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, stride=128, window=None, files=None, samples_per_file=512):\n",
    "        self.files = files\n",
    "\n",
    "        max_length = 1024\n",
    "        if window is not None:\n",
    "            max_length = window\n",
    "\n",
    "        dataset = []\n",
    "        df = pd.read_csv('ETTh1-clean.csv', header=None)\n",
    "        df = df.dropna() # i think axis=1 drops columns\n",
    "        # drop any columns with str\n",
    "        df = df.drop(df.select_dtypes(['object']), axis=1)\n",
    "\n",
    "        # # only get first row\n",
    "        # df = df.iloc[:, 0]\n",
    "\n",
    "        norm_df = (df - df.min()) * (50_257-2) / ( df.max() - df.min() )\n",
    "        n_cols = 1#norm_df.shape[1]\n",
    "\n",
    "        tokens = norm_df.values.flatten().astype(int)\n",
    "        \n",
    "        # Create sequences with sliding window\n",
    "        samples = 0\n",
    "        for i in range(0, len(tokens) - max_length, stride):\n",
    "            sequence = tokens[i:i + max_length]\n",
    "            if len(sequence) == max_length:\n",
    "                input_sequence = np.array(sequence[:-n_cols])#, dtype=np.int64) # dont include the last token\n",
    "                target_sequence = np.array(sequence[n_cols:])#, dtype=np.int64) # dont include the first token\n",
    "\n",
    "                dataset.append({\n",
    "                    'input_ids': input_sequence,\n",
    "                    'labels': target_sequence\n",
    "                })\n",
    "            samples += 1\n",
    "            if samples > samples_per_file: # 1024\n",
    "                print('max samples from file')\n",
    "                break\n",
    "        print('samples:', samples)\n",
    "\n",
    "        self.data = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # print(item['input_ids'].shape, item['labels'].shape, torch.ones_like(torch.tensor(item['input_ids'])).shape)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']).to(device),\n",
    "            'labels': torch.tensor(item['labels']).to(device),\n",
    "            'attention_mask': torch.ones_like(torch.tensor(item['input_ids'])).to(device)\n",
    "        }\n",
    "    \n",
    "    def min(self):\n",
    "        if len(self.files) == 1:\n",
    "            return self.min_val\n",
    "        else:\n",
    "            raise Exception(\"Multiple files, use min_val from each file\")\n",
    "    \n",
    "    def max(self):\n",
    "        if len(self.files) == 1:\n",
    "            return self.max_val\n",
    "        else:\n",
    "            raise Exception(\"Multiple files, use max_val from each file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## every id for a given token is embedded to vector of this size\n",
    "n_embd            = 768        # GPT-2\n",
    "n_head            = 4#12         # GPT-2\n",
    "n_layer           = 8#12         # GPT-2\n",
    "dropout           = 0.1        # GPT-2\n",
    "\n",
    "learning_rate     = 2.5e-4     # GPT-2\n",
    "vocab_size        = 50_257     # GPT-2 50_257\n",
    "block_size        = 512       # GPT-2 (context) ## N tokens in sequence\n",
    "\n",
    "batch_size        = 2\n",
    "# max_iters         = 512\n",
    "eval_interval     = 512\n",
    "# eval_iters        = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "\n",
    "class GPTConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size=1024,\n",
    "        vocab_size=50_257,\n",
    "        n_embd=768,\n",
    "        n_head=8,\n",
    "        n_layer=8,\n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "class GPTModelForTrainer(PreTrainedModel):\n",
    "    config_class = GPTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # keep model inside because we cant pass it in\n",
    "        self.model = GPTModel(\n",
    "                    vocab_size=self.vocab_size,\n",
    "                    n_embd=self.n_embd,\n",
    "                    block_size=self.block_size,\n",
    "                    n_layer=self.n_layer,\n",
    "                    n_head=self.n_head,\n",
    "                    dropout=self.dropout\n",
    "                )\n",
    "        \n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        logits = self.model(input_ids)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :, :].contiguous()\n",
    "            shift_labels = labels[..., :].contiguous().long()\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                          shift_labels.view(-1))\n",
    "            \n",
    "        return CausalLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) array of indices in the current context\n",
    "        max_new_tokens: number of tokens to generate\n",
    "        temperature: control randomness (1.0 = neutral, < 1.0 = more deterministic, > 1.0 = more random)\n",
    "        top_k: limit sampling to top k most likely tokens (None = no limit)\n",
    "        \"\"\"\n",
    "        # Make sure model is in eval mode\n",
    "        self.eval()\n",
    "        \n",
    "        # Loop until we generate all requested tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If context length exceeds block_size, crop it\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = self(idx_cond)['logits']\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Optionally crop logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 949\n",
      "train size: 854\n",
      "eval size: 95\n"
     ]
    }
   ],
   "source": [
    "dataset = GPTDataset(window=block_size, samples_per_file=4096)\n",
    "\n",
    "# split the dataset in train and validation set\n",
    "torch.manual_seed(42)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "print('train size:', len(train_dataset))\n",
    "print('eval size:', len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11986' max='12810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11986/12810 19:46 < 01:21, 10.10 it/s, Epoch 28.07/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>7.341900</td>\n",
       "      <td>6.040496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>5.374100</td>\n",
       "      <td>5.058834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>4.598700</td>\n",
       "      <td>4.522678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>3.874700</td>\n",
       "      <td>3.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>2.921200</td>\n",
       "      <td>3.143767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3072</td>\n",
       "      <td>1.621200</td>\n",
       "      <td>2.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3584</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>1.239754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4096</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.775584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4608</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.534580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.450203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5632</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.427328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6144</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.418576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6656</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.443807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7168</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.432696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.392536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8192</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.372233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8704</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.345669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9216</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.326439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9728</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.318550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10240</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.316636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10752</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.302120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11264</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.297372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11776</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.289821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "output_dir = Path('models') / time.strftime(\"%y-%m-%d_%H\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='gpt_model',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_steps=eval_interval,\n",
    "    save_steps=eval_interval,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=learning_rate,\n",
    "    # fp16=True,  # if you want to use mixed precision training\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=eval_interval,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# Wrap the model\n",
    "config = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model_for_trainer = GPTModelForTrainer(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_trainer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights\n",
    "# Replace 'checkpoint-XXX' with the specific checkpoint you want to load\n",
    "checkpoint_path = \"./gpt_model/checkpoint-512\"  # or whatever your output_dir/checkpoint-XXX is\n",
    "\n",
    "import os\n",
    "checkpoint_path = max([f\"gpt_model/{f}\" for f in os.listdir('gpt_model') if os.path.isdir(f\"gpt_model/{f}\")], key=os.path.getmtime)\n",
    "\n",
    "print(checkpoint_path)\n",
    "model_for_trainer = GPTModelForTrainer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Put model in evaluation mode if you're going to use it for inference\n",
    "model_for_trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO good enough for now\n",
    "device = 'cpu'\n",
    "\n",
    "# test_dataset = GPTDataset(files=['exchange_rate.txt'], window=block_size)\n",
    "\n",
    "# Generate text\n",
    "output = model_for_trainer.generate(\n",
    "    eval_dataset[0]['input_ids'].unsqueeze(0),\n",
    "    # max_length=512,\n",
    "    max_new_tokens=1,\n",
    "    # temperature=0.7,\n",
    "    # num_return_sequences=1,\n",
    "    # pad_token_id=vocab_size - 1,\n",
    "    # do_sample=True\n",
    ")\n",
    "\n",
    "torch.set_printoptions(profile='full')\n",
    "print('start')\n",
    "print('real',eval_dataset[0]['labels'][:32])\n",
    "print('pred',output[0][1:33])\n",
    "print('middle (1020:1050)')\n",
    "print('real',eval_dataset[0]['labels'][1000:1050])\n",
    "print('pred',output[0][1001:1051])\n",
    "print('end')\n",
    "print('real',eval_dataset[0]['labels'][-32:])\n",
    "print('pred',output[0][-32:])\n",
    "torch.set_printoptions(profile='default')\n",
    "\n",
    "# create scatter plot\n",
    "n = 100\n",
    "m = 1\n",
    "\n",
    "# not sure how to line up the real and pred when using different size. so i will just change dataset size\n",
    "real = torch.full_like(torch.zeros(n), eval_dataset[0]['labels'][-1])\n",
    "pred = torch.zeros(n)\n",
    "\n",
    "pbar = tqdm(range(0, n, m), total=n)\n",
    "for i in pbar:\n",
    "    output = model_for_trainer.generate(\n",
    "        eval_dataset[0]['input_ids'][:block_size].unsqueeze(0),\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "\n",
    "    for j in range(m): # -1\n",
    "        pred[i+j] = output[j][-1].cpu() # have to do this to get the value\n",
    "    pbar.update(m)\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(range(n), real, label='real')\n",
    "plt.scatter(range(n), pred, label='pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame({'Predictions': pred})\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(y='Predictions', data=df)\n",
    "plt.title(f\"Distribution of  Predictions\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"real: shape: {real.shape}, goal val: {real[0]}\")\n",
    "print(f\"min: {min(pred)}, max: {max(pred)}, mean: {torch.mean(pred)}, std: {torch.std(pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scatter plot\n",
    "n = 100\n",
    "m = 1\n",
    "\n",
    "if n >= len(eval_dataset)-1:\n",
    "    n = len(eval_dataset)-1\n",
    "\n",
    "# not sure how to line up the real and pred when using different size. so i will just change dataset size\n",
    "real = torch.zeros(n)\n",
    "pred = torch.zeros(n)\n",
    "\n",
    "pbar = tqdm(range(0, n, m), total=n)\n",
    "for i in pbar:\n",
    "    output = model_for_trainer.generate(\n",
    "        eval_dataset[i]['input_ids'][:block_size].unsqueeze(0),\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "\n",
    "    for j in range(m): # -1\n",
    "        real[i+j] = eval_dataset[i]['labels'][-1]\n",
    "        pred[i+j] = output[j][-1].cpu() # have to do this to get the value\n",
    "    pbar.update(m)\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(range(n), real, label='real')\n",
    "plt.scatter(range(n), pred, label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.mean((real - pred) ** 2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim percent\n",
    "max_diff = torch.max(torch.abs(real))  # Maximum possible difference\n",
    "mae = torch.mean(torch.abs(real - pred))\n",
    "\n",
    "print(100 * (1 - mae/max_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO output dir. dataset vars. samples=None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
